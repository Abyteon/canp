# é¡¹ç›®åº“ä½¿ç”¨æŒ‡å—

## ğŸ“š æ¦‚è¿°

CANPé¡¹ç›®ä½¿ç”¨äº†å¤šä¸ªä¼˜ç§€çš„Ruståº“æ¥å®ç°é«˜æ€§èƒ½çš„CANæ€»çº¿æ•°æ®å¤„ç†ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å„ä¸ªåº“çš„ä½¿ç”¨æ–¹æ³•ã€æœ€ä½³å®è·µå’Œåœ¨é¡¹ç›®ä¸­çš„åº”ç”¨ã€‚

## ğŸ”§ æ ¸å¿ƒåº“è¯¦è§£

### 1. memmap2 - å†…å­˜æ˜ å°„

#### åŸºæœ¬ç”¨æ³•

```rust
use memmap2::Mmap;
use std::fs::File;

// åŸºæœ¬å†…å­˜æ˜ å°„
pub fn map_file_simple(path: &str) -> Result<Mmap> {
    let file = File::open(path)?;
    let mmap = unsafe { Mmap::map(&file)? };
    Ok(mmap)
}

// åœ¨CANPä¸­çš„åº”ç”¨
pub struct MemoryMappedFile {
    mmap: Arc<Mmap>,
    file_path: PathBuf,
}

impl MemoryMappedFile {
    pub fn new(file_path: PathBuf) -> Result<Self> {
        let file = File::open(&file_path)?;
        let mmap = unsafe { Mmap::map(&file)? };
        
        Ok(Self {
            mmap: Arc::new(mmap),
            file_path,
        })
    }
    
    // é›¶æ‹·è´è¯»å–
    pub fn read_slice(&self, offset: usize, length: usize) -> Option<&[u8]> {
        if offset + length <= self.mmap.len() {
            Some(&self.mmap[offset..offset + length])
        } else {
            None
        }
    }
    
    // æ‰¹é‡è¯»å–
    pub fn read_batch(&self, batch_size: usize) -> Vec<&[u8]> {
        let mut batches = Vec::new();
        let mut offset = 0;
        
        while offset < self.mmap.len() {
            let remaining = self.mmap.len() - offset;
            let read_size = std::cmp::min(batch_size, remaining);
            
            if let Some(slice) = self.read_slice(offset, read_size) {
                batches.push(slice);
                offset += read_size;
            } else {
                break;
            }
        }
        
        batches
    }
}
```

#### é«˜çº§ç‰¹æ€§

```rust
// å¯å†™å†…å­˜æ˜ å°„
pub fn create_writable_mmap(path: &str, size: usize) -> Result<MmapMut> {
    let file = OpenOptions::new()
        .read(true)
        .write(true)
        .create(true)
        .open(path)?;
    
    file.set_len(size as u64)?;
    let mmap = unsafe { MmapMut::map_mut(&file)? };
    Ok(mmap)
}

// å†…å­˜æ˜ å°„ç¼“å­˜
pub struct MmapCache {
    cache: Arc<RwLock<HashMap<PathBuf, Arc<Mmap>>>>,
    max_entries: usize,
}

impl MmapCache {
    pub fn new(max_entries: usize) -> Self {
        Self {
            cache: Arc::new(RwLock::new(HashMap::new())),
            max_entries,
        }
    }
    
    pub fn get_or_load(&self, path: &Path) -> Result<Arc<Mmap>> {
        // å…ˆæ£€æŸ¥ç¼“å­˜
        if let Some(mmap) = self.cache.read().unwrap().get(path) {
            return Ok(Arc::clone(mmap));
        }
        
        // åŠ è½½æ–‡ä»¶
        let mmap = Arc::new(MemoryMappedFile::new(path.to_path_buf())?.mmap);
        
        // æ›´æ–°ç¼“å­˜
        let mut cache = self.cache.write().unwrap();
        if cache.len() >= self.max_entries {
            // ç®€å•çš„LRUç­–ç•¥ï¼šç§»é™¤ç¬¬ä¸€ä¸ªæ¡ç›®
            if let Some(key) = cache.keys().next().cloned() {
                cache.remove(&key);
            }
        }
        cache.insert(path.to_path_buf(), Arc::clone(&mmap));
        
        Ok(mmap)
    }
}
```

### 2. bytes - é›¶æ‹·è´ç¼“å†²åŒº

#### åŸºæœ¬ç”¨æ³•

```rust
use bytes::{Bytes, BytesMut, Buf, BufMut};

// åŸºæœ¬ç¼“å†²åŒºæ“ä½œ
pub struct BufferManager {
    buffer: BytesMut,
}

impl BufferManager {
    pub fn new(capacity: usize) -> Self {
        Self {
            buffer: BytesMut::with_capacity(capacity),
        }
    }
    
    // å†™å…¥æ•°æ®
    pub fn write_data(&mut self, data: &[u8]) {
        self.buffer.put_slice(data);
    }
    
    // è¯»å–æ•°æ®
    pub fn read_data(&mut self, length: usize) -> Option<Bytes> {
        if self.buffer.len() >= length {
            Some(self.buffer.split_to(length).freeze())
        } else {
            None
        }
    }
    
    // æŸ¥çœ‹æ•°æ®è€Œä¸æ¶ˆè´¹
    pub fn peek_data(&self, length: usize) -> Option<&[u8]> {
        if self.buffer.len() >= length {
            Some(&self.buffer[..length])
        } else {
            None
        }
    }
}
```

#### åœ¨CANPä¸­çš„åº”ç”¨

```rust
// é›¶æ‹·è´å†…å­˜ç¼“å†²åŒº
pub struct MutableMemoryBuffer {
    buffer: BytesMut,
}

impl MutableMemoryBuffer {
    pub fn new(capacity: usize) -> Self {
        Self {
            buffer: BytesMut::with_capacity(capacity),
        }
    }
    
    pub fn with_data(data: &[u8]) -> Self {
        Self {
            buffer: BytesMut::from(data),
        }
    }
    
    // è·å–å¯å†™åˆ‡ç‰‡
    pub fn get_mut_slice(&mut self) -> &mut [u8] {
        &mut self.buffer[..]
    }
    
    // è·å–ä¸å¯å˜åˆ‡ç‰‡
    pub fn get_slice(&self) -> &[u8] {
        &self.buffer[..]
    }
    
    // è½¬æ¢ä¸ºä¸å¯å˜Bytes
    pub fn freeze(self) -> Bytes {
        self.buffer.freeze()
    }
    
    // æ‰©å±•ç¼“å†²åŒº
    pub fn extend_from_slice(&mut self, data: &[u8]) {
        self.buffer.extend_from_slice(data);
    }
    
    // æ¸…ç©ºç¼“å†²åŒº
    pub fn clear(&mut self) {
        self.buffer.clear();
    }
    
    // è·å–å®¹é‡
    pub fn capacity(&self) -> usize {
        self.buffer.capacity()
    }
    
    // è·å–é•¿åº¦
    pub fn len(&self) -> usize {
        self.buffer.len()
    }
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    pub fn is_empty(&self) -> bool {
        self.buffer.is_empty()
    }
}
```

### 3. lock_pool - å¯¹è±¡æ± 

#### åŸºæœ¬ç”¨æ³•

```rust
use lock_pool::LockPool;

// åŸºæœ¬å¯¹è±¡æ± 
pub struct BufferPool {
    pool: LockPool<BytesMut, 64, 512>,
}

impl BufferPool {
    pub fn new() -> Self {
        Self {
            pool: LockPool::new(),
        }
    }
    
    // è·å–ç¼“å†²åŒº
    pub fn get_buffer(&self, size: usize) -> BytesMut {
        self.pool.get(size)
    }
    
    // è¿”å›ç¼“å†²åŒº
    pub fn return_buffer(&self, buffer: BytesMut) {
        self.pool.put(buffer);
    }
}
```

#### åœ¨CANPä¸­çš„åº”ç”¨

```rust
// åˆ†å±‚è§£å‹ç¼“å†²åŒºæ± 
pub struct DecompressBufferPools {
    pools: Vec<Arc<LockPool<BytesMut, 64, 512>>>,
}

impl DecompressBufferPools {
    pub fn new() -> Self {
        // åˆ›å»ºä¸åŒå¤§å°çš„æ± 
        let pool_sizes = vec![1024, 4096, 16384, 65536, 262144];
        let pools = pool_sizes
            .into_iter()
            .map(|size| Arc::new(LockPool::new()))
            .collect();
            
        Self { pools }
    }
    
    // æ ¹æ®å¤§å°é€‰æ‹©åˆé€‚çš„æ± 
    pub fn get_buffer(&self, size: usize) -> BytesMut {
        for pool in &self.pools {
            if pool.capacity() >= size {
                return pool.get(size);
            }
        }
        
        // å¦‚æœæ²¡æœ‰åˆé€‚çš„æ± ï¼Œåˆ›å»ºæ–°çš„ç¼“å†²åŒº
        BytesMut::with_capacity(size)
    }
    
    // æ‰¹é‡è·å–ç¼“å†²åŒº
    pub fn get_buffers_batch(&self, sizes: &[usize]) -> Vec<BytesMut> {
        sizes.iter().map(|&size| self.get_buffer(size)).collect()
    }
}
```

### 4. lru - LRUç¼“å­˜

#### åŸºæœ¬ç”¨æ³•

```rust
use lru::LruCache;

// åŸºæœ¬LRUç¼“å­˜
pub struct FileCache {
    cache: Arc<RwLock<LruCache<String, Arc<Mmap>>>>,
}

impl FileCache {
    pub fn new(capacity: usize) -> Self {
        Self {
            cache: Arc::new(RwLock::new(LruCache::new(capacity))),
        }
    }
    
    // è·å–æ–‡ä»¶
    pub fn get(&self, path: &str) -> Option<Arc<Mmap>> {
        let mut cache = self.cache.write().unwrap();
        cache.get(path).map(|mmap| Arc::clone(mmap))
    }
    
    // æ’å…¥æ–‡ä»¶
    pub fn insert(&self, path: String, mmap: Arc<Mmap>) {
        let mut cache = self.cache.write().unwrap();
        cache.put(path, mmap);
    }
    
    // æ£€æŸ¥æ˜¯å¦å­˜åœ¨
    pub fn contains(&self, path: &str) -> bool {
        let cache = self.cache.read().unwrap();
        cache.contains(path)
    }
}
```

#### åœ¨CANPä¸­çš„åº”ç”¨

```rust
// å†…å­˜æ˜ å°„ç¼“å­˜
pub struct MmapCache {
    cache: Arc<RwLock<lru::LruCache<String, Arc<Mmap>>>>,
}

impl MmapCache {
    pub fn new(capacity: usize) -> Self {
        Self {
            cache: Arc::new(RwLock::new(lru::LruCache::new(capacity))),
        }
    }
    
    pub fn get_mmap(&self, path: &str) -> Option<Arc<Mmap>> {
        let mut cache = self.cache.write().unwrap();
        
        // æ£€æŸ¥ç¼“å­˜
        if let Some(mmap) = cache.get(path) {
            return Some(Arc::clone(mmap));
        }
        
        // åŠ è½½æ–‡ä»¶
        if let Ok(mmap) = self.load_file(path) {
            let mmap_arc = Arc::new(mmap);
            cache.put(path.to_string(), Arc::clone(&mmap_arc));
            Some(mmap_arc)
        } else {
            None
        }
    }
    
    fn load_file(&self, path: &str) -> Result<Mmap> {
        let file = File::open(path)?;
        unsafe { Ok(Mmap::map(&file)?) }
    }
    
    // é¢„åŠ è½½æ–‡ä»¶
    pub fn preload(&self, paths: &[String]) {
        for path in paths {
            let _ = self.get_mmap(path);
        }
    }
    
    // æ¸…ç†ç¼“å­˜
    pub fn clear(&self) {
        let mut cache = self.cache.write().unwrap();
        cache.clear();
    }
}
```

### 5. can-dbc - DBCè§£æ

#### åŸºæœ¬ç”¨æ³•

```rust
use can_dbc::{Dbc, Signal, Message};

// åŸºæœ¬DBCè§£æ
pub struct DbcParser {
    dbc: Option<Dbc>,
}

impl DbcParser {
    pub fn new() -> Self {
        Self { dbc: None }
    }
    
    // è§£æDBCæ–‡ä»¶
    pub fn parse_file(&mut self, path: &str) -> Result<()> {
        let content = std::fs::read_to_string(path)?;
        self.dbc = Some(Dbc::from_str(&content)?);
        Ok(())
    }
    
    // è·å–æ¶ˆæ¯
    pub fn get_message(&self, id: u32) -> Option<&Message> {
        self.dbc.as_ref()?.messages.iter().find(|msg| msg.id == id)
    }
    
    // è·å–ä¿¡å·
    pub fn get_signal(&self, message_id: u32, signal_name: &str) -> Option<&Signal> {
        let message = self.get_message(message_id)?;
        message.signals.iter().find(|sig| sig.name == signal_name)
    }
}
```

#### åœ¨CANPä¸­çš„åº”ç”¨

```rust
// DBCç®¡ç†å™¨
pub struct DbcManager {
    dbc_cache: Arc<RwLock<HashMap<PathBuf, DbcCacheEntry>>>,
    stats: Arc<RwLock<DbcParsingStats>>,
}

impl DbcManager {
    pub fn new() -> Self {
        Self {
            dbc_cache: Arc::new(RwLock::new(HashMap::new())),
            stats: Arc::new(RwLock::new(DbcParsingStats::new())),
        }
    }
    
    // åŠ è½½DBCæ–‡ä»¶
    pub async fn load_dbc(&self, path: &Path) -> Result<Arc<Dbc>> {
        let path_key = path.to_path_buf();
        
        // æ£€æŸ¥ç¼“å­˜
        {
            let cache = self.dbc_cache.read().unwrap();
            if let Some(entry) = cache.get(&path_key) {
                if !entry.is_expired() {
                    return Ok(Arc::clone(&entry.dbc));
                }
            }
        }
        
        // è§£æDBCæ–‡ä»¶
        let content = tokio::fs::read_to_string(path).await?;
        let dbc = Dbc::from_str(&content)?;
        let dbc_arc = Arc::new(dbc);
        
        // æ›´æ–°ç¼“å­˜
        {
            let mut cache = self.dbc_cache.write().unwrap();
            let entry = DbcCacheEntry::new(Arc::clone(&dbc_arc));
            cache.insert(path_key, entry);
        }
        
        // æ›´æ–°ç»Ÿè®¡
        {
            let mut stats = self.stats.write().unwrap();
            stats.files_loaded += 1;
        }
        
        Ok(dbc_arc)
    }
    
    // æå–ä¿¡å·å€¼
    pub fn extract_signal_value(
        &self,
        dbc: &Dbc,
        message_id: u32,
        signal_name: &str,
        data: &[u8],
    ) -> Result<f64> {
        let message = dbc.messages
            .iter()
            .find(|msg| msg.id == message_id)
            .ok_or_else(|| anyhow!("æ¶ˆæ¯ID {} æœªæ‰¾åˆ°", message_id))?;
            
        let signal = message.signals
            .iter()
            .find(|sig| sig.name == signal_name)
            .ok_or_else(|| anyhow!("ä¿¡å· {} æœªæ‰¾åˆ°", signal_name))?;
            
        // æå–ä¿¡å·å€¼
        let value = self.extract_signal_from_data(signal, data)?;
        Ok(value)
    }
    
    // ä»æ•°æ®ä¸­æå–ä¿¡å·å€¼
    fn extract_signal_from_data(&self, signal: &Signal, data: &[u8]) -> Result<f64> {
        let start_bit = signal.start_bit as usize;
        let length = signal.bit_size as usize;
        
        // ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿ
        if data.len() * 8 < start_bit + length {
            return Err(anyhow!("æ•°æ®é•¿åº¦ä¸è¶³"));
        }
        
        // æå–ä½å€¼
        let mut value = 0u64;
        for i in 0..length {
            let bit_pos = start_bit + i;
            let byte_index = bit_pos / 8;
            let bit_index = bit_pos % 8;
            
            if byte_index < data.len() {
                let bit_value = (data[byte_index] >> bit_index) & 1;
                value |= (bit_value as u64) << i;
            }
        }
        
        // åº”ç”¨å› å­å’Œåç§»
        let raw_value = value as f64;
        let scaled_value = raw_value * signal.factor + signal.offset;
        
        Ok(scaled_value)
    }
}
```

### 6. arrow - åˆ—å¼å­˜å‚¨

#### åŸºæœ¬ç”¨æ³•

```rust
use arrow::array::{ArrayRef, Float64Array, UInt32Array, StringArray};
use arrow::record_batch::RecordBatch;
use arrow::datatypes::{Field, Schema, DataType};

// åŸºæœ¬Arrowæ•°ç»„æ“ä½œ
pub struct ArrowDataBuilder {
    schema: Schema,
    arrays: Vec<ArrayRef>,
}

impl ArrowDataBuilder {
    pub fn new() -> Self {
        let schema = Schema::new(vec![
            Field::new("timestamp", DataType::UInt32, false),
            Field::new("message_id", DataType::UInt32, false),
            Field::new("signal_name", DataType::Utf8, false),
            Field::new("value", DataType::Float64, false),
        ]);
        
        Self {
            schema,
            arrays: Vec::new(),
        }
    }
    
    // æ·»åŠ æ•°æ®
    pub fn add_data(
        &mut self,
        timestamps: Vec<u32>,
        message_ids: Vec<u32>,
        signal_names: Vec<String>,
        values: Vec<f64>,
    ) -> Result<()> {
        let timestamp_array = Arc::new(UInt32Array::from(timestamps));
        let message_id_array = Arc::new(UInt32Array::from(message_ids));
        let signal_name_array = Arc::new(StringArray::from(signal_names));
        let value_array = Arc::new(Float64Array::from(values));
        
        self.arrays = vec![
            timestamp_array,
            message_id_array,
            signal_name_array,
            value_array,
        ];
        
        Ok(())
    }
    
    // åˆ›å»ºè®°å½•æ‰¹æ¬¡
    pub fn build(self) -> Result<RecordBatch> {
        RecordBatch::try_new(Arc::new(self.schema), self.arrays)
            .map_err(|e| anyhow!("åˆ›å»ºè®°å½•æ‰¹æ¬¡å¤±è´¥: {}", e))
    }
}
```

#### åœ¨CANPä¸­çš„åº”ç”¨

```rust
// åˆ—å¼å­˜å‚¨å†™å…¥å™¨
pub struct ColumnarStorageWriter {
    schema: Arc<Schema>,
    record_batches: Vec<RecordBatch>,
    batch_size: usize,
}

impl ColumnarStorageWriter {
    pub fn new() -> Self {
        let schema = Schema::new(vec![
            Field::new("timestamp", DataType::UInt64, false),
            Field::new("message_id", DataType::UInt32, false),
            Field::new("signal_name", DataType::Utf8, false),
            Field::new("raw_value", DataType::UInt64, false),
            Field::new("scaled_value", DataType::Float64, false),
            Field::new("unit", DataType::Utf8, true),
        ]);
        
        Self {
            schema: Arc::new(schema),
            record_batches: Vec::new(),
            batch_size: 10000,
        }
    }
    
    // æ·»åŠ CANæ•°æ®
    pub fn add_can_data(
        &mut self,
        timestamp: u64,
        message_id: u32,
        signal_name: String,
        raw_value: u64,
        scaled_value: f64,
        unit: Option<String>,
    ) -> Result<()> {
        // åˆ›å»ºå•è¡Œæ•°æ®
        let batch = RecordBatch::try_new(
            Arc::clone(&self.schema),
            vec![
                Arc::new(UInt64Array::from(vec![timestamp])),
                Arc::new(UInt32Array::from(vec![message_id])),
                Arc::new(StringArray::from(vec![signal_name])),
                Arc::new(UInt64Array::from(vec![raw_value])),
                Arc::new(Float64Array::from(vec![scaled_value])),
                Arc::new(StringArray::from(vec![unit])),
            ],
        )?;
        
        self.record_batches.push(batch);
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
        if self.record_batches.len() >= self.batch_size {
            self.flush()?;
        }
        
        Ok(())
    }
    
    // æ‰¹é‡æ·»åŠ æ•°æ®
    pub fn add_batch_data(
        &mut self,
        timestamps: Vec<u64>,
        message_ids: Vec<u32>,
        signal_names: Vec<String>,
        raw_values: Vec<u64>,
        scaled_values: Vec<f64>,
        units: Vec<Option<String>>,
    ) -> Result<()> {
        let batch = RecordBatch::try_new(
            Arc::clone(&self.schema),
            vec![
                Arc::new(UInt64Array::from(timestamps)),
                Arc::new(UInt32Array::from(message_ids)),
                Arc::new(StringArray::from(signal_names)),
                Arc::new(UInt64Array::from(raw_values)),
                Arc::new(Float64Array::from(scaled_values)),
                Arc::new(StringArray::from(units)),
            ],
        )?;
        
        self.record_batches.push(batch);
        
        if self.record_batches.len() >= self.batch_size {
            self.flush()?;
        }
        
        Ok(())
    }
    
    // åˆ·æ–°æ•°æ®åˆ°ç£ç›˜
    pub fn flush(&mut self) -> Result<()> {
        if self.record_batches.is_empty() {
            return Ok(());
        }
        
        // åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        let combined_batch = self.combine_batches()?;
        
        // å†™å…¥Parquetæ–‡ä»¶
        self.write_parquet(&combined_batch)?;
        
        // æ¸…ç©ºæ‰¹æ¬¡
        self.record_batches.clear();
        
        Ok(())
    }
    
    // åˆå¹¶æ‰¹æ¬¡
    fn combine_batches(&self) -> Result<RecordBatch> {
        if self.record_batches.len() == 1 {
            return Ok(self.record_batches[0].clone());
        }
        
        // åˆå¹¶å¤šä¸ªæ‰¹æ¬¡
        let mut combined_arrays = Vec::new();
        let num_columns = self.schema.fields().len();
        
        for col_idx in 0..num_columns {
            let mut column_data = Vec::new();
            
            for batch in &self.record_batches {
                column_data.push(batch.column(col_idx).clone());
            }
            
            // è¿æ¥æ•°ç»„
            let combined_array = arrow::compute::concat(&column_data)?;
            combined_arrays.push(combined_array);
        }
        
        RecordBatch::try_new(Arc::clone(&self.schema), combined_arrays)
            .map_err(|e| anyhow!("åˆå¹¶æ‰¹æ¬¡å¤±è´¥: {}", e))
    }
    
    // å†™å…¥Parquetæ–‡ä»¶
    fn write_parquet(&self, batch: &RecordBatch) -> Result<()> {
        use parquet::arrow::arrow_writer::ArrowWriter;
        use std::fs::File;
        
        let file = File::create("output.parquet")?;
        let mut writer = ArrowWriter::try_new(file, Arc::clone(&self.schema), None)?;
        
        writer.write(batch)?;
        writer.close()?;
        
        Ok(())
    }
}
```

### 7. flate2 - å‹ç¼©è§£å‹

#### åŸºæœ¬ç”¨æ³•

```rust
use flate2::read::GzDecoder;
use flate2::write::GzEncoder;
use flate2::Compression;
use std::io::{Read, Write};

// åŸºæœ¬å‹ç¼©è§£å‹
pub struct CompressionHandler;

impl CompressionHandler {
    // å‹ç¼©æ•°æ®
    pub fn compress_data(data: &[u8]) -> Result<Vec<u8>> {
        let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
        encoder.write_all(data)?;
        Ok(encoder.finish()?)
    }
    
    // è§£å‹æ•°æ®
    pub fn decompress_data(data: &[u8]) -> Result<Vec<u8>> {
        let mut decoder = GzDecoder::new(data);
        let mut result = Vec::new();
        decoder.read_to_end(&mut result)?;
        Ok(result)
    }
}
```

#### åœ¨CANPä¸­çš„åº”ç”¨

```rust
// æ•°æ®è§£å‹å™¨
pub struct DataDecompressor {
    buffer_pool: Arc<DecompressBufferPools>,
}

impl DataDecompressor {
    pub fn new(buffer_pool: Arc<DecompressBufferPools>) -> Self {
        Self { buffer_pool }
    }
    
    // è§£å‹æ•°æ®
    pub async fn decompress_data(&self, compressed_data: &[u8]) -> Result<Vec<u8>> {
        let mut decoder = GzDecoder::new(compressed_data);
        let mut buffer = self.buffer_pool.get_buffer(compressed_data.len() * 4);
        
        let bytes_read = decoder.read(&mut buffer)?;
        buffer.truncate(bytes_read);
        
        Ok(buffer.to_vec())
    }
    
    // æ‰¹é‡è§£å‹
    pub async fn decompress_batch(&self, compressed_chunks: &[&[u8]]) -> Result<Vec<Vec<u8>>> {
        let mut results = Vec::new();
        
        for chunk in compressed_chunks {
            let decompressed = self.decompress_data(chunk).await?;
            results.push(decompressed);
        }
        
        Ok(results)
    }
    
    // æµå¼è§£å‹
    pub async fn decompress_stream(
        &self,
        mut compressed_stream: impl Read + Send,
    ) -> Result<Vec<u8>> {
        let mut compressed_data = Vec::new();
        compressed_stream.read_to_end(&mut compressed_data)?;
        
        self.decompress_data(&compressed_data).await
    }
}
```

### 8. metrics - æŒ‡æ ‡æ”¶é›†

#### åŸºæœ¬ç”¨æ³•

```rust
use metrics::{counter, gauge, histogram};

// åŸºæœ¬æŒ‡æ ‡æ”¶é›†
pub struct MetricsCollector;

impl MetricsCollector {
    // è®°å½•è®¡æ•°å™¨
    pub fn increment_processed_files() {
        counter!("files_processed_total", 1);
    }
    
    // è®°å½•ä»ªè¡¨
    pub fn set_memory_usage(bytes: usize) {
        gauge!("memory_usage_bytes", bytes as f64);
    }
    
    // è®°å½•ç›´æ–¹å›¾
    pub fn record_processing_time(duration: Duration) {
        histogram!("processing_time_seconds", duration.as_secs_f64());
    }
    
    // è®°å½•å¸¦æ ‡ç­¾çš„æŒ‡æ ‡
    pub fn record_error(error_type: &str) {
        counter!("errors_total", 1, "type" => error_type.to_string());
    }
}
```

#### åœ¨CANPä¸­çš„åº”ç”¨

```rust
// æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
pub struct PerformanceMetrics {
    start_time: Instant,
}

impl PerformanceMetrics {
    pub fn new() -> Self {
        Self {
            start_time: Instant::now(),
        }
    }
    
    // è®°å½•æ–‡ä»¶å¤„ç†æŒ‡æ ‡
    pub fn record_file_processing(&self, file_size: usize, processing_time: Duration) {
        counter!("files_processed_total", 1);
        histogram!("file_processing_time_seconds", processing_time.as_secs_f64());
        histogram!("file_size_bytes", file_size as f64);
        
        let throughput = file_size as f64 / processing_time.as_secs_f64();
        histogram!("processing_throughput_bytes_per_second", throughput);
    }
    
    // è®°å½•å†…å­˜ä½¿ç”¨æŒ‡æ ‡
    pub fn record_memory_usage(&self, current_usage: usize, peak_usage: usize) {
        gauge!("memory_usage_current_bytes", current_usage as f64);
        gauge!("memory_usage_peak_bytes", peak_usage as f64);
        
        let usage_percentage = (current_usage as f64 / peak_usage as f64) * 100.0;
        gauge!("memory_usage_percentage", usage_percentage);
    }
    
    // è®°å½•é”™è¯¯æŒ‡æ ‡
    pub fn record_error(&self, error_type: &str, error_message: &str) {
        counter!("errors_total", 1, "type" => error_type.to_string());
        counter!("error_messages_total", 1, "message" => error_message.to_string());
    }
    
    // è®°å½•ç³»ç»ŸæŒ‡æ ‡
    pub fn record_system_metrics(&self) {
        let uptime = self.start_time.elapsed();
        gauge!("system_uptime_seconds", uptime.as_secs_f64());
        
        // CPUä½¿ç”¨ç‡
        if let Ok(cpu_usage) = self.get_cpu_usage() {
            gauge!("cpu_usage_percentage", cpu_usage);
        }
        
        // ç£ç›˜IO
        if let Ok(disk_io) = self.get_disk_io() {
            gauge!("disk_read_bytes", disk_io.read_bytes as f64);
            gauge!("disk_write_bytes", disk_io.write_bytes as f64);
        }
    }
    
    fn get_cpu_usage(&self) -> Result<f64> {
        // å®ç°CPUä½¿ç”¨ç‡è·å–é€»è¾‘
        Ok(0.0)
    }
    
    fn get_disk_io(&self) -> Result<DiskIO> {
        // å®ç°ç£ç›˜IOè·å–é€»è¾‘
        Ok(DiskIO { read_bytes: 0, write_bytes: 0 })
    }
}

struct DiskIO {
    read_bytes: u64,
    write_bytes: u64,
}
```

## ğŸ”§ åº“é›†æˆæœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†é›†æˆ

```rust
// ç»Ÿä¸€çš„é”™è¯¯å¤„ç†
pub struct LibraryErrorHandler;

impl LibraryErrorHandler {
    // å¤„ç†memmap2é”™è¯¯
    pub fn handle_mmap_error(error: memmap2::Error) -> ProcessingError {
        match error {
            memmap2::Error::Io(io_error) => ProcessingError::Io(io_error),
            memmap2::Error::InvalidArgument => ProcessingError::InvalidFormat("æ— æ•ˆçš„å†…å­˜æ˜ å°„å‚æ•°".to_string()),
        }
    }
    
    // å¤„ç†can-dbcé”™è¯¯
    pub fn handle_dbc_error(error: can_dbc::Error) -> ProcessingError {
        match error {
            can_dbc::Error::ParseError(msg) => ProcessingError::Parse { message: msg, line: 0 },
            can_dbc::Error::IoError(io_error) => ProcessingError::Io(io_error),
        }
    }
    
    // å¤„ç†arrowé”™è¯¯
    pub fn handle_arrow_error(error: arrow::error::ArrowError) -> ProcessingError {
        ProcessingError::InvalidFormat(format!("Arrowé”™è¯¯: {}", error))
    }
}
```

### 2. æ€§èƒ½ä¼˜åŒ–é›†æˆ

```rust
// åº“æ€§èƒ½ä¼˜åŒ–
pub struct LibraryOptimizer;

impl LibraryOptimizer {
    // ä¼˜åŒ–memmap2ä½¿ç”¨
    pub fn optimize_mmap_usage(file_path: &Path) -> Result<Mmap> {
        // ä½¿ç”¨å¤§é¡µé¢æ”¯æŒ
        #[cfg(target_os = "linux")]
        {
            // å°è¯•ä½¿ç”¨å¤§é¡µé¢
            if let Ok(mmap) = unsafe { Mmap::map_with_options(
                &File::open(file_path)?,
                memmap2::MmapOptions::new().huge(Some(memmap2::HugePage::Size2MB))
            )} {
                return Ok(mmap);
            }
        }
        
        // å›é€€åˆ°æ ‡å‡†æ˜ å°„
        let file = File::open(file_path)?;
        unsafe { Ok(Mmap::map(&file)?) }
    }
    
    // ä¼˜åŒ–bytesç¼“å†²åŒº
    pub fn optimize_buffer_usage(buffer: &mut BytesMut, expected_size: usize) {
        if buffer.capacity() < expected_size {
            buffer.reserve(expected_size - buffer.capacity());
        }
    }
    
    // ä¼˜åŒ–å¯¹è±¡æ± é…ç½®
    pub fn optimize_pool_config(data_size: usize) -> usize {
        // æ ¹æ®æ•°æ®å¤§å°é€‰æ‹©åˆé€‚çš„æ± å¤§å°
        match data_size {
            0..=1024 => 64,
            1025..=4096 => 32,
            4097..=16384 => 16,
            _ => 8,
        }
    }
}
```

### 3. ç›‘æ§é›†æˆ

```rust
// åº“ç›‘æ§é›†æˆ
pub struct LibraryMonitor;

impl LibraryMonitor {
    // ç›‘æ§memmap2ä½¿ç”¨
    pub fn monitor_mmap_usage(mmap: &Mmap) {
        gauge!("mmap_size_bytes", mmap.len() as f64);
        counter!("mmap_operations_total", 1);
    }
    
    // ç›‘æ§bytesç¼“å†²åŒºä½¿ç”¨
    pub fn monitor_buffer_usage(buffer: &BytesMut) {
        gauge!("buffer_capacity_bytes", buffer.capacity() as f64);
        gauge!("buffer_used_bytes", buffer.len() as f64);
        
        let utilization = buffer.len() as f64 / buffer.capacity() as f64;
        gauge!("buffer_utilization_percentage", utilization * 100.0);
    }
    
    // ç›‘æ§å¯¹è±¡æ± ä½¿ç”¨
    pub fn monitor_pool_usage(pool: &LockPool<BytesMut, 64, 512>) {
        gauge!("pool_capacity", pool.capacity() as f64);
        gauge!("pool_available", pool.available() as f64);
        
        let utilization = (pool.capacity() - pool.available()) as f64 / pool.capacity() as f64;
        gauge!("pool_utilization_percentage", utilization * 100.0);
    }
}
```

## ğŸ“š æ€»ç»“

CANPé¡¹ç›®é€šè¿‡åˆç†ä½¿ç”¨è¿™äº›ä¼˜ç§€çš„Ruståº“ï¼Œå®ç°äº†é«˜æ€§èƒ½çš„CANæ€»çº¿æ•°æ®å¤„ç†ç³»ç»Ÿï¼š

- **memmap2**: æä¾›é›¶æ‹·è´æ–‡ä»¶è®¿é—®
- **bytes**: é«˜æ•ˆçš„ç¼“å†²åŒºç®¡ç†
- **lock_pool**: å¯¹è±¡æ± æ¨¡å¼å‡å°‘åˆ†é…å¼€é”€
- **lru**: LRUç¼“å­˜æé«˜è®¿é—®æ•ˆç‡
- **can-dbc**: ä¸“ä¸šçš„DBCæ–‡ä»¶è§£æ
- **arrow**: é«˜æ€§èƒ½åˆ—å¼æ•°æ®å­˜å‚¨
- **flate2**: æ•°æ®å‹ç¼©è§£å‹
- **metrics**: ç³»ç»ŸæŒ‡æ ‡æ”¶é›†

å…³é”®è¦ç‚¹ï¼š
- åˆç†é…ç½®åº“å‚æ•°ä»¥ä¼˜åŒ–æ€§èƒ½
- å®ç°ç»Ÿä¸€çš„é”™è¯¯å¤„ç†æœºåˆ¶
- æ·»åŠ é€‚å½“çš„ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†
- æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„åº“åŠŸèƒ½
- æŒç»­ä¼˜åŒ–åº“çš„ä½¿ç”¨æ–¹å¼ 