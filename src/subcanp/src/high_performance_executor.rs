//! # é«˜æ€§èƒ½ä»»åŠ¡æ‰§è¡Œå™¨ (High-Performance Task Executor)
//!
//! ä¸“é—¨ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†ä»»åŠ¡è®¾è®¡çš„é«˜æ€§èƒ½æ‰§è¡Œå™¨ï¼Œç»“åˆäº†ç¤¾åŒºæœ€ä½³å®è·µï¼š
//! - Tokioå¼‚æ­¥è¿è¡Œæ—¶å¤„ç†IOå¯†é›†å‹ä»»åŠ¡
//! - Rayonæ•°æ®å¹¶è¡Œå¤„ç†CPUå¯†é›†å‹ä»»åŠ¡  
//! - å¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…ï¼ˆMPMCï¼‰æ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½
//! - å·¥ä½œçªƒå–ç®—æ³•æé«˜èµ„æºåˆ©ç”¨ç‡
//! - èƒŒå‹æ§åˆ¶é˜²æ­¢å†…å­˜æº¢å‡º

use anyhow::Result;
use crossbeam::channel; // æ–°å¢ï¼šå¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…é€šé“
use num_cpus;
use rayon::prelude::*;
use std::future::Future;
use std::pin::Pin;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::{Arc, RwLock};
use std::time::{Duration, Instant};
use tokio::sync::{Semaphore, oneshot};
use tokio::task::JoinHandle;
use tracing::{debug, error, info, warn};

/// ä»»åŠ¡ç±»å‹æšä¸¾
/// åŸºäºtokioå’Œrayonå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum TaskType {
    /// IOå¯†é›†å‹ä»»åŠ¡ï¼ˆæ–‡ä»¶æ˜ å°„ã€ç½‘ç»œIOã€ç£ç›˜IOï¼‰
    IoIntensive,
    /// CPUå¯†é›†å‹ä»»åŠ¡ï¼ˆè§£å‹ã€ç¼–ç è§£ç ã€æ•°å­¦è®¡ç®—ï¼‰
    CpuIntensive,
    /// æ··åˆå‹ä»»åŠ¡ï¼ˆæ•°æ®è§£æã€æ ¼å¼è½¬æ¢ï¼‰
    Mixed,
    /// é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆé”™è¯¯å¤„ç†ã€ç›‘æ§ï¼‰
    HighPriority,
    /// è‡ªå®šä¹‰ä»»åŠ¡ç±»å‹
    Custom(u32),
}

impl TaskType {
    /// è·å–ä»»åŠ¡çš„å»ºè®®çº¿ç¨‹æ± ç±»å‹
    pub fn suggested_pool(&self) -> &'static str {
        match self {
            TaskType::IoIntensive => "io",
            TaskType::CpuIntensive => "cpu",
            TaskType::Mixed => "mixed",
            TaskType::HighPriority => "priority",
            TaskType::Custom(_) => "custom",
        }
    }

    /// è·å–ä»»åŠ¡çš„æƒé‡ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡ï¼‰
    pub fn weight(&self) -> u32 {
        match self {
            TaskType::IoIntensive => 1,
            TaskType::CpuIntensive => 2,
            TaskType::Mixed => 3,
            TaskType::HighPriority => 10,
            TaskType::Custom(weight) => *weight,
        }
    }

    /// è·å–ä»»åŠ¡çš„å»ºè®®è¶…æ—¶æ—¶é—´
    pub fn suggested_timeout(&self) -> Duration {
        match self {
            TaskType::HighPriority => Duration::from_secs(60), // é«˜ä¼˜å…ˆçº§ä»»åŠ¡çŸ­è¶…æ—¶
            TaskType::IoIntensive => Duration::from_secs(300), // IOä»»åŠ¡æ ‡å‡†è¶…æ—¶
            TaskType::CpuIntensive => Duration::from_secs(600), // CPUä»»åŠ¡é•¿è¶…æ—¶
            TaskType::Mixed => Duration::from_secs(450),       // æ··åˆä»»åŠ¡ä¸­ç­‰è¶…æ—¶
            TaskType::Custom(_) => Duration::from_secs(300),   // è‡ªå®šä¹‰ä»»åŠ¡é»˜è®¤è¶…æ—¶
        }
    }

    /// è·å–ä»»åŠ¡çš„å»ºè®®æ‰¹é‡å¤§å°
    pub fn suggested_batch_size(&self) -> usize {
        match self {
            TaskType::HighPriority => 5,  // é«˜ä¼˜å…ˆçº§ä»»åŠ¡å°æ‰¹é‡
            TaskType::CpuIntensive => 15, // CPUå¯†é›†å‹ä»»åŠ¡å¤§æ‰¹é‡
            TaskType::Mixed => 10,        // æ··åˆä»»åŠ¡ä¸­ç­‰æ‰¹é‡
            TaskType::IoIntensive => 8,   // IOä»»åŠ¡ä¸­ç­‰æ‰¹é‡
            TaskType::Custom(_) => 10,    // è‡ªå®šä¹‰ä»»åŠ¡é»˜è®¤æ‰¹é‡
        }
    }
}

/// ä»»åŠ¡ä¼˜å…ˆçº§
#[derive(Debug, Clone, Copy, Ord, PartialOrd, Eq, PartialEq)]
pub enum Priority {
    Low = 0,
    Normal = 1,
    High = 2,
    Critical = 3,
}

/// ä»»åŠ¡å…ƒæ•°æ®
#[derive(Debug)]
pub struct TaskMetadata {
    pub id: u64,
    pub task_type: TaskType,
    pub priority: Priority,
    pub created_at: Instant,
    pub estimated_duration: Option<Duration>,
    pub description: String,
}

/// ä»»åŠ¡æ‰§è¡Œç»“æœ
#[derive(Debug)]
pub struct TaskResult<T> {
    pub metadata: TaskMetadata,
    pub result: Result<T>,
    pub execution_time: Duration,
    pub worker_id: Option<usize>,
}

/// æ‰§è¡Œå™¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆMPMCä¼˜åŒ–ç‰ˆæœ¬ï¼‰
#[derive(Debug, Clone)]
pub struct ExecutorStats {
    pub total_tasks: usize,
    pub completed_tasks: usize,
    pub failed_tasks: usize,
    // ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    pub io_tasks: usize,
    pub cpu_tasks: usize,
    pub mixed_tasks: usize,
    pub high_priority_tasks: usize,

    // æ—¶é—´ç»Ÿè®¡
    pub average_execution_time: Duration,
    pub total_io_time: Duration,
    pub total_cpu_time: Duration,

    // å·¥ä½œçº¿ç¨‹ç»Ÿè®¡
    pub active_io_workers: usize,
    pub active_cpu_workers: usize,
    pub queue_length: usize,

    // é”™è¯¯ç»Ÿè®¡
    pub timeout_tasks: usize,
    pub queue_full_rejections: usize,
    pub worker_restarts: usize,
}

impl ExecutorStats {
    /// è®¡ç®—ä»»åŠ¡æˆåŠŸç‡
    pub fn success_rate(&self) -> f64 {
        if self.total_tasks == 0 {
            0.0
        } else {
            self.completed_tasks as f64 / self.total_tasks as f64
        }
    }

    /// è®¡ç®—å¹³å‡IOä»»åŠ¡æ—¶é—´
    pub fn average_io_time(&self) -> Duration {
        if self.io_tasks == 0 {
            Duration::from_nanos(0)
        } else {
            Duration::from_nanos(self.total_io_time.as_nanos() as u64 / self.io_tasks as u64)
        }
    }

    /// è®¡ç®—å¹³å‡CPUä»»åŠ¡æ—¶é—´
    pub fn average_cpu_time(&self) -> Duration {
        if self.cpu_tasks == 0 {
            Duration::from_nanos(0)
        } else {
            Duration::from_nanos(self.total_cpu_time.as_nanos() as u64 / self.cpu_tasks as u64)
        }
    }
}

/// å·¥ä½œçªƒå–é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
type BoxedTask = Pin<Box<dyn Future<Output = ()> + Send + 'static>>;
type BoxedCpuTask = Box<dyn FnOnce() -> Result<()> + Send + 'static>;

/// é«˜æ€§èƒ½ä»»åŠ¡æ‰§è¡Œå™¨ï¼ˆMPMCç‰ˆæœ¬ï¼‰
///
/// ç»“åˆäº†å¤šç§ä¼˜ç§€çš„å¹¶å‘æ¨¡å¼ï¼š
/// - Tokioçš„å¼‚æ­¥è¿è¡Œæ—¶ç”¨äºIOä»»åŠ¡
/// - Rayonçš„å·¥ä½œçªƒå–ç”¨äºCPUä»»åŠ¡
/// - Crossbeam MPMCé€šé“å®ç°çœŸæ­£çš„å¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…
/// - è‡ªå®šä¹‰ä¼˜å…ˆçº§é˜Ÿåˆ—ç”¨äºä»»åŠ¡è°ƒåº¦
/// - èƒŒå‹æ§åˆ¶é˜²æ­¢å†…å­˜æº¢å‡º
pub struct HighPerformanceExecutor {
    /// é…ç½®å‚æ•°
    config: ExecutorConfig,

    /// IOä»»åŠ¡é˜Ÿåˆ—ï¼ˆå¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…ï¼‰
    io_task_tx: channel::Sender<(TaskMetadata, BoxedTask)>,

    /// CPUä»»åŠ¡é˜Ÿåˆ—ï¼ˆå¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…ï¼‰
    cpu_task_tx: channel::Sender<(TaskMetadata, BoxedCpuTask)>,

    /// é«˜ä¼˜å…ˆçº§ä»»åŠ¡é˜Ÿåˆ—ï¼ˆå¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…ï¼‰
    priority_task_tx: channel::Sender<(TaskMetadata, BoxedTask)>,

    /// ä»»åŠ¡IDç”Ÿæˆå™¨
    task_id_counter: Arc<AtomicUsize>,

    /// æ‰§è¡Œå™¨ç»Ÿè®¡
    stats: Arc<RwLock<ExecutorStats>>,

    /// èƒŒå‹æ§åˆ¶ä¿¡å·é‡
    backpressure_semaphore: Arc<Semaphore>,

    /// å·¥ä½œçº¿ç¨‹å¥æŸ„ï¼ˆç”¨äºä¼˜é›…å…³é—­ï¼‰
    worker_handles: Arc<RwLock<Vec<JoinHandle<()>>>>,

    /// å…³é—­ä¿¡å·
    shutdown_tx: Option<oneshot::Sender<()>>,
}

/// æ‰§è¡Œå™¨é…ç½®ï¼ˆMPMCä¼˜åŒ–ç‰ˆæœ¬ï¼‰
#[derive(Debug, Clone)]
pub struct ExecutorConfig {
    /// IOå·¥ä½œçº¿ç¨‹æ•°é‡ï¼ˆå¤šæ¶ˆè´¹è€…ï¼‰
    pub io_worker_count: usize,
    /// CPUå·¥ä½œçº¿ç¨‹æ•°é‡ï¼ˆå¤šæ¶ˆè´¹è€…ï¼‰
    pub cpu_worker_count: usize,
    /// æœ€å¤§é˜Ÿåˆ—é•¿åº¦ï¼ˆèƒŒå‹æ§åˆ¶ï¼‰
    pub max_queue_length: usize,
    /// ä»»åŠ¡è¶…æ—¶æ—¶é—´
    pub task_timeout: Duration,
    /// ç»Ÿè®¡æ›´æ–°é—´éš”
    pub stats_update_interval: Duration,
    /// æ˜¯å¦å¯ç”¨å·¥ä½œçªƒå–
    pub enable_work_stealing: bool,
    /// CPUä»»åŠ¡æ‰¹é‡å¤§å°
    pub cpu_batch_size: usize,
    /// æ˜¯å¦ä½¿ç”¨æœ‰ç•Œé˜Ÿåˆ—
    pub bounded_queue: bool,
    /// é˜Ÿåˆ—å®¹é‡ï¼ˆæœ‰ç•Œé˜Ÿåˆ—æ—¶ç”Ÿæ•ˆï¼‰
    pub queue_capacity: usize,
}

impl Default for ExecutorConfig {
    fn default() -> Self {
        let cpu_cores = num_cpus::get();
        Self {
            io_worker_count: cpu_cores * 2, // IOå¯†é›†å‹ç”¨æ›´å¤šçº¿ç¨‹
            cpu_worker_count: cpu_cores,    // CPUå¯†é›†å‹ç”¨CPUæ ¸å¿ƒæ•°
            max_queue_length: 10000,
            task_timeout: Duration::from_secs(300), // 5åˆ†é’Ÿè¶…æ—¶
            stats_update_interval: Duration::from_secs(10),
            enable_work_stealing: true,
            cpu_batch_size: 100,
            bounded_queue: false, // é»˜è®¤ä½¿ç”¨æ— ç•Œé˜Ÿåˆ—
            queue_capacity: 1000, // æœ‰ç•Œé˜Ÿåˆ—å®¹é‡
        }
    }
}

impl HighPerformanceExecutor {
    /// åˆ›å»ºæ–°çš„é«˜æ€§èƒ½æ‰§è¡Œå™¨ï¼ˆMPMCæ¨¡å¼ï¼‰
    pub fn new(config: ExecutorConfig) -> Self {
        info!("ğŸš€ åˆå§‹åŒ–é«˜æ€§èƒ½æ‰§è¡Œå™¨ (MPMCæ¨¡å¼)");

        // åˆ›å»ºMPMCé€šé“ - åŸºäºcrossbeamæœ€ä½³å®è·µ
        let (io_task_tx, io_task_rx) = if config.bounded_queue {
            channel::bounded(config.queue_capacity)
        } else {
            channel::unbounded()
        };

        let (cpu_task_tx, cpu_task_rx) = if config.bounded_queue {
            channel::bounded(config.queue_capacity)
        } else {
            channel::unbounded()
        };

        let (priority_task_tx, priority_task_rx) = if config.bounded_queue {
            channel::bounded(config.queue_capacity / 4) // ä¼˜å…ˆçº§é˜Ÿåˆ—è¾ƒå°
        } else {
            channel::unbounded()
        };

        let (shutdown_tx, shutdown_rx) = oneshot::channel();

        let backpressure_semaphore = Arc::new(Semaphore::new(config.max_queue_length));
        let stats = Arc::new(RwLock::new(ExecutorStats {
            total_tasks: 0,
            completed_tasks: 0,
            failed_tasks: 0,
            io_tasks: 0,
            cpu_tasks: 0,
            mixed_tasks: 0,
            high_priority_tasks: 0,
            average_execution_time: Duration::from_millis(0),
            total_io_time: Duration::from_nanos(0),
            total_cpu_time: Duration::from_nanos(0),
            active_io_workers: config.io_worker_count,
            active_cpu_workers: config.cpu_worker_count,
            queue_length: 0,
            timeout_tasks: 0,
            queue_full_rejections: 0,
            worker_restarts: 0,
        }));

        let executor = Self {
            config: config.clone(),
            io_task_tx,
            cpu_task_tx,
            priority_task_tx,
            task_id_counter: Arc::new(AtomicUsize::new(1)),
            stats: Arc::clone(&stats),
            backpressure_semaphore,
            worker_handles: Arc::new(RwLock::new(Vec::new())),
            shutdown_tx: Some(shutdown_tx),
        };

        // å¯åŠ¨MPMCå·¥ä½œçº¿ç¨‹
        executor.start_mpmc_workers(io_task_rx, cpu_task_rx, priority_task_rx, shutdown_rx);

        info!(
            "ğŸš€ é«˜æ€§èƒ½æ‰§è¡Œå™¨å·²å¯åŠ¨ (MPMC) - IOå·¥ä½œçº¿ç¨‹: {}, CPUå·¥ä½œçº¿ç¨‹: {}",
            config.io_worker_count, config.cpu_worker_count
        );

        executor
    }

    /// å¯åŠ¨æ‰€æœ‰MPMCå·¥ä½œçº¿ç¨‹
    fn start_mpmc_workers(
        &self,
        io_task_rx: channel::Receiver<(TaskMetadata, BoxedTask)>,
        cpu_task_rx: channel::Receiver<(TaskMetadata, BoxedCpuTask)>,
        priority_task_rx: channel::Receiver<(TaskMetadata, BoxedTask)>,
        shutdown_rx: oneshot::Receiver<()>,
    ) {
        let mut handles = Vec::new();

        // å¯åŠ¨å¤šä¸ªIOå·¥ä½œæ¶ˆè´¹è€…
        for worker_id in 0..self.config.io_worker_count {
            let handle = self.start_io_worker(worker_id, io_task_rx.clone());
            handles.push(handle);
        }

        // å¯åŠ¨å¤šä¸ªCPUå·¥ä½œæ¶ˆè´¹è€…
        for worker_id in 0..self.config.cpu_worker_count {
            let handle = self.start_cpu_worker(worker_id, cpu_task_rx.clone());
            handles.push(handle);
        }

        // å¯åŠ¨é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†å™¨
        let priority_handle = self.start_priority_worker(priority_task_rx);
        handles.push(priority_handle);

        // å¯åŠ¨ç»Ÿè®¡æ”¶é›†å™¨
        let stats_handle = self.start_stats_collector();
        handles.push(stats_handle);

        // å¯åŠ¨å…³é—­ç›‘å¬å™¨
        let shutdown_handle = self.start_shutdown_listener(shutdown_rx);
        handles.push(shutdown_handle);

        // ä¿å­˜å·¥ä½œçº¿ç¨‹å¥æŸ„
        *self.worker_handles.write().unwrap() = handles;

        info!(
            "âœ… å·²å¯åŠ¨ {} ä¸ªIOå·¥ä½œçº¿ç¨‹å’Œ {} ä¸ªCPUå·¥ä½œçº¿ç¨‹",
            self.config.io_worker_count, self.config.cpu_worker_count
        );
    }

    /// å¯åŠ¨IOå·¥ä½œçº¿ç¨‹ï¼ˆMPMCæ¶ˆè´¹è€…ï¼‰
    fn start_io_worker(
        &self,
        worker_id: usize,
        io_task_rx: channel::Receiver<(TaskMetadata, BoxedTask)>,
    ) -> JoinHandle<()> {
        let stats = Arc::clone(&self.stats);
        let semaphore = Arc::clone(&self.backpressure_semaphore);

        tokio::spawn(async move {
            info!("ğŸ”§ IOå·¥ä½œçº¿ç¨‹ {} å¯åŠ¨", worker_id);

            // MPMCæ¶ˆè´¹è€…å¾ªç¯ - åŸºäºcrossbeamå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
            while let Ok((metadata, task)) = io_task_rx.recv() {
                let start_time = Instant::now();
                let task_id = metadata.id;

                debug!("ğŸ“¥ IOå·¥ä½œçº¿ç¨‹ {} å¼€å§‹æ‰§è¡Œä»»åŠ¡: {}", worker_id, task_id);

                // æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ï¼Œå¸¦è¶…æ—¶æ§åˆ¶
                let timeout_duration = metadata.task_type.suggested_timeout();
                let result = tokio::time::timeout(timeout_duration, task).await;

                let execution_time = start_time.elapsed();

                // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                {
                    let mut stats = stats.write().unwrap();
                    stats.io_tasks += 1;
                    stats.total_io_time += execution_time;

                    // match result {
                    //     Ok(Ok(_)) => {
                    //         stats.completed_tasks += 1;
                    //         debug!("âœ… IOå·¥ä½œçº¿ç¨‹ {} å®Œæˆä»»åŠ¡ {}", worker_id, task_id);
                    //     }
                    //     Ok(Err(_)) => {
                    //         stats.failed_tasks += 1;
                    //         error!("âŒ IOå·¥ä½œçº¿ç¨‹ {} ä»»åŠ¡ {} å¤±è´¥", worker_id, task_id);
                    //     }
                    //     Err(_) => {
                    //         stats.failed_tasks += 1;
                    //         stats.timeout_tasks += 1;
                    //         error!("â° IOå·¥ä½œçº¿ç¨‹ {} ä»»åŠ¡ {} è¶…æ—¶", worker_id, task_id);
                    //     }
                    // }

                    // æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´
                    let total_completed = stats.completed_tasks + stats.failed_tasks;
                    if total_completed > 0 {
                        let current_avg_nanos = stats.average_execution_time.as_nanos() as u128;
                        let new_avg_nanos = (current_avg_nanos * (total_completed - 1) as u128
                            + execution_time.as_nanos() as u128)
                            / total_completed as u128;
                        stats.average_execution_time = Duration::from_nanos(new_avg_nanos as u64);
                    }
                }

                // é‡Šæ”¾èƒŒå‹ä¿¡å·é‡
                semaphore.add_permits(1);
            }

            warn!("ğŸ”§ IOå·¥ä½œçº¿ç¨‹ {} é€€å‡º", worker_id);
        })
    }

    /// å¯åŠ¨CPUå·¥ä½œçº¿ç¨‹ï¼ˆMPMCæ¶ˆè´¹è€…ï¼‰
    fn start_cpu_worker(
        &self,
        worker_id: usize,
        cpu_task_rx: channel::Receiver<(TaskMetadata, BoxedCpuTask)>,
    ) -> JoinHandle<()> {
        let stats = Arc::clone(&self.stats);
        let semaphore = Arc::clone(&self.backpressure_semaphore);
        let batch_size = self.config.cpu_batch_size;

        tokio::spawn(async move {
            info!("ğŸ’ª CPUå·¥ä½œçº¿ç¨‹ {} å¯åŠ¨", worker_id);

            let mut task_batch = Vec::with_capacity(batch_size);

            // MPMCæ¶ˆè´¹è€…å¾ªç¯ - æ”¯æŒæ‰¹é‡å¤„ç†
            while let Ok((metadata, task)) = cpu_task_rx.recv() {
                task_batch.push((metadata, task));

                // æ‰¹é‡å¤„ç†CPUä»»åŠ¡ä»¥æé«˜æ•ˆç‡
                if task_batch.len() >= batch_size {
                    Self::process_cpu_batch_worker(worker_id, &mut task_batch, &stats, &semaphore)
                        .await;
                }

                // æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šä»»åŠ¡å¯ä»¥ç«‹å³å¤„ç†
                while let Ok((metadata, task)) = cpu_task_rx.try_recv() {
                    task_batch.push((metadata, task));
                    if task_batch.len() >= batch_size {
                        Self::process_cpu_batch_worker(
                            worker_id,
                            &mut task_batch,
                            &stats,
                            &semaphore,
                        )
                        .await;
                    }
                }

                // å¤„ç†å‰©ä½™ä»»åŠ¡
                if !task_batch.is_empty() {
                    Self::process_cpu_batch_worker(worker_id, &mut task_batch, &stats, &semaphore)
                        .await;
                }
            }

            warn!("ğŸ’ª CPUå·¥ä½œçº¿ç¨‹ {} é€€å‡º", worker_id);
        })
    }

    /// å¤„ç†CPUä»»åŠ¡æ‰¹æ¬¡ï¼ˆå•ä¸ªå·¥ä½œçº¿ç¨‹ç‰ˆæœ¬ï¼‰
    async fn process_cpu_batch_worker(
        worker_id: usize,
        task_batch: &mut Vec<(TaskMetadata, BoxedCpuTask)>,
        stats: &Arc<RwLock<ExecutorStats>>,
        semaphore: &Arc<Semaphore>,
    ) {
        let batch = std::mem::take(task_batch);
        let batch_size = batch.len();

        debug!(
            "ğŸ”§ CPUå·¥ä½œçº¿ç¨‹ {} å¤„ç†æ‰¹æ¬¡ {} ä¸ªä»»åŠ¡",
            worker_id, batch_size
        );

        // ä½¿ç”¨Rayonè¿›è¡Œå¹¶è¡Œå¤„ç† - åŸºäºrayonå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
        let results: Vec<(u64, Result<Duration>)> = batch
            .into_par_iter()
            .map(|(metadata, task)| {
                let start_time = Instant::now();
                let task_id = metadata.id;

                debug!("ğŸ”§ å¼€å§‹æ‰§è¡ŒCPUä»»åŠ¡: {} (å·¥ä½œçº¿ç¨‹ {})", task_id, worker_id);

                let result = task().map(|_| start_time.elapsed());

                match &result {
                    Ok(execution_time) => {
                        debug!(
                            "âœ… CPUä»»åŠ¡å®Œæˆ: {} - è€—æ—¶: {:?} (å·¥ä½œçº¿ç¨‹ {})",
                            task_id, execution_time, worker_id
                        );
                    }
                    Err(e) => {
                        error!(
                            "âŒ CPUä»»åŠ¡å¤±è´¥: {} - {} (å·¥ä½œçº¿ç¨‹ {})",
                            task_id, e, worker_id
                        );
                    }
                }

                (task_id, result)
            })
            .collect();

        // æ”¶é›†ç»“æœå¹¶æ›´æ–°ç»Ÿè®¡
        let mut completed = 0;
        let mut failed = 0;
        let mut total_time = Duration::from_nanos(0);

        for (_, result) in results {
            match result {
                Ok(execution_time) => {
                    completed += 1;
                    total_time += execution_time;
                }
                Err(_) => {
                    failed += 1;
                }
            }
        }

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        {
            let mut stats = stats.write().unwrap();
            stats.completed_tasks += completed;
            stats.failed_tasks += failed;
            stats.cpu_tasks += completed + failed;
            stats.total_cpu_time += total_time;

            // æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´
            let total_completed = stats.completed_tasks + stats.failed_tasks;
            if total_completed > 0 {
                let current_avg_nanos = stats.average_execution_time.as_nanos() as u128;
                let new_avg_nanos = (current_avg_nanos
                    * (total_completed - (completed + failed)) as u128
                    + total_time.as_nanos() as u128)
                    / total_completed as u128;
                stats.average_execution_time = Duration::from_nanos(new_avg_nanos as u64);
            }
        }

        // é‡Šæ”¾èƒŒå‹ä¿¡å·é‡
        semaphore.add_permits(batch_size);

        debug!(
            "âœ… CPUå·¥ä½œçº¿ç¨‹ {} å®Œæˆæ‰¹æ¬¡å¤„ç†: æˆåŠŸ {}, å¤±è´¥ {}",
            worker_id, completed, failed
        );
    }

    /// å¯åŠ¨é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†å™¨
    fn start_priority_worker(
        &self,
        priority_task_rx: channel::Receiver<(TaskMetadata, BoxedTask)>,
    ) -> JoinHandle<()> {
        let stats = Arc::clone(&self.stats);

        tokio::spawn(async move {
            info!("âš¡ é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†å™¨å·²å¯åŠ¨");

            while let Ok((metadata, task)) = priority_task_rx.recv() {
                let stats = Arc::clone(&stats);
                let task_id = metadata.id;
                let start_time = Instant::now();

                debug!("ğŸ”¥ å¼€å§‹æ‰§è¡Œé«˜ä¼˜å…ˆçº§ä»»åŠ¡: {}", task_id);

                // é«˜ä¼˜å…ˆçº§ä»»åŠ¡ç«‹å³æ‰§è¡Œï¼Œä¸å—èƒŒå‹æ§åˆ¶
                let timeout_duration = Duration::from_secs(60);
                let result = tokio::time::timeout(timeout_duration, task).await;

                let execution_time = start_time.elapsed();

                // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                {
                    let mut stats = stats.write().unwrap();
                    stats.high_priority_tasks += 1;

                    // match result {
                    //     Ok(Ok(_)) => {
                    //         stats.completed_tasks += 1;
                    //         debug!(
                    //             "âœ… é«˜ä¼˜å…ˆçº§ä»»åŠ¡å®Œæˆ: {} - è€—æ—¶: {:?}",
                    //             task_id, execution_time
                    //         );
                    //     }
                    //     Ok(Err(_)) => {
                    //         stats.failed_tasks += 1;
                    //         error!("âŒ é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤±è´¥: {}", task_id);
                    //     }
                    //     Err(_) => {
                    //         stats.failed_tasks += 1;
                    //         stats.timeout_tasks += 1;
                    //         error!("â° é«˜ä¼˜å…ˆçº§ä»»åŠ¡è¶…æ—¶: {}", task_id);
                    //     }
                    // }
                }
            }

            warn!("âš¡ é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†å™¨é€€å‡º");
        })
    }

    /// å¯åŠ¨ç»Ÿè®¡æ”¶é›†å™¨
    fn start_stats_collector(&self) -> JoinHandle<()> {
        let stats = Arc::clone(&self.stats);
        let interval = self.config.stats_update_interval;

        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(interval);

            loop {
                ticker.tick().await;

                let stats = stats.read().unwrap().clone();
                info!(
                    "ğŸ“Š æ‰§è¡Œå™¨çŠ¶æ€ (MPMC) - æ€»ä»»åŠ¡: {}, å®Œæˆ: {} ({:.1}%), å¤±è´¥: {}, è¶…æ—¶: {}, å¹³å‡è€—æ—¶: {:?}, IOå·¥ä½œçº¿ç¨‹: {}, CPUå·¥ä½œçº¿ç¨‹: {}",
                    stats.total_tasks,
                    stats.completed_tasks,
                    stats.success_rate() * 100.0,
                    stats.failed_tasks,
                    stats.timeout_tasks,
                    stats.average_execution_time,
                    stats.active_io_workers,
                    stats.active_cpu_workers
                );
            }
        })
    }

    /// å¯åŠ¨å…³é—­ç›‘å¬å™¨
    fn start_shutdown_listener(&self, shutdown_rx: oneshot::Receiver<()>) -> JoinHandle<()> {
        tokio::spawn(async move {
            if let Ok(_) = shutdown_rx.await {
                info!("ğŸ›‘ æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ‰§è¡Œå™¨å³å°†å…³é—­");
            }
        })
    }

    /// æäº¤IOå¯†é›†å‹ä»»åŠ¡ï¼ˆMPMCä¼˜åŒ–ï¼‰
    pub async fn submit_io_task<F, T>(
        &self,
        description: String,
        priority: Priority,
        task: F,
    ) -> Result<u64>
    where
        F: Future<Output = Result<T>> + Send + 'static,
        T: Send + 'static,
    {
        let task_id = self.task_id_counter.fetch_add(1, Ordering::SeqCst) as u64;

        let metadata = TaskMetadata {
            id: task_id,
            task_type: TaskType::IoIntensive,
            priority,
            created_at: Instant::now(),
            estimated_duration: None,
            description,
        };

        // åŒ…è£…ä»»åŠ¡ä»¥å¤„ç†ç»“æœ
        let boxed_task = Box::pin(async move {
            match task.await {
                Ok(_result) => {
                    debug!("âœ… IOä»»åŠ¡æˆåŠŸå®Œæˆ: {}", task_id);
                }
                Err(e) => {
                    error!("âŒ IOä»»åŠ¡æ‰§è¡Œå¤±è´¥: {} - {}", task_id, e);
                }
            }
        });

        // æäº¤å‰è·å–èƒŒå‹è®¸å¯
        let _permit = self.backpressure_semaphore.acquire().await.unwrap();

        // æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©é˜Ÿåˆ—
        let sender = if priority >= Priority::High {
            &self.priority_task_tx
        } else {
            &self.io_task_tx
        };

        // ä½¿ç”¨MPMCé€šé“å‘é€ä»»åŠ¡
        sender
            .send((metadata, boxed_task))
            .map_err(|_| anyhow::anyhow!("Failed to submit IO task"))?;

        // æ›´æ–°ç»Ÿè®¡
        {
            let mut stats = self.stats.write().unwrap();
            stats.total_tasks += 1;
        }

        Ok(task_id)
    }

    /// æäº¤CPUå¯†é›†å‹ä»»åŠ¡ï¼ˆMPMCä¼˜åŒ–ï¼‰
    pub async fn submit_cpu_task<F, T>(
        &self,
        description: String,
        priority: Priority,
        task: F,
    ) -> Result<u64>
    where
        F: FnOnce() -> Result<T> + Send + 'static,
        T: Send + 'static,
    {
        let task_id = self.task_id_counter.fetch_add(1, Ordering::SeqCst) as u64;

        let metadata = TaskMetadata {
            id: task_id,
            task_type: TaskType::CpuIntensive,
            priority,
            created_at: Instant::now(),
            estimated_duration: None,
            description,
        };

        // åŒ…è£…ä»»åŠ¡
        let boxed_task = Box::new(move || match task() {
            Ok(_result) => {
                debug!("âœ… CPUä»»åŠ¡æˆåŠŸå®Œæˆ: {}", task_id);
                Ok(())
            }
            Err(e) => {
                error!("âŒ CPUä»»åŠ¡æ‰§è¡Œå¤±è´¥: {} - {}", task_id, e);
                Err(e)
            }
        });

        // æäº¤å‰è·å–èƒŒå‹è®¸å¯
        let _permit = self.backpressure_semaphore.acquire().await.unwrap();

        // ä½¿ç”¨MPMCé€šé“å‘é€ä»»åŠ¡
        self.cpu_task_tx
            .send((metadata, boxed_task))
            .map_err(|_| anyhow::anyhow!("Failed to submit CPU task"))?;

        // æ›´æ–°ç»Ÿè®¡
        {
            let mut stats = self.stats.write().unwrap();
            stats.total_tasks += 1;
        }

        Ok(task_id)
    }

    /// æ‰¹é‡æäº¤IOä»»åŠ¡
    pub async fn submit_io_batch<F, T>(&self, tasks: Vec<(String, Priority, F)>) -> Result<Vec<u64>>
    where
        F: Future<Output = Result<T>> + Send + 'static,
        T: Send + 'static,
    {
        let mut task_ids = Vec::with_capacity(tasks.len());

        for (description, priority, task) in tasks {
            let task_id = self.submit_io_task(description, priority, task).await?;
            task_ids.push(task_id);
        }

        Ok(task_ids)
    }

    /// æ‰¹é‡æäº¤CPUä»»åŠ¡
    pub async fn submit_cpu_batch<F, T>(
        &self,
        tasks: Vec<(String, Priority, F)>,
    ) -> Result<Vec<u64>>
    where
        F: FnOnce() -> Result<T> + Send + 'static,
        T: Send + 'static,
    {
        let mut task_ids = Vec::with_capacity(tasks.len());

        for (description, priority, task) in tasks {
            let task_id = self.submit_cpu_task(description, priority, task).await?;
            task_ids.push(task_id);
        }

        Ok(task_ids)
    }

    /// è·å–æ‰§è¡Œå™¨ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> ExecutorStats {
        self.stats.read().unwrap().clone()
    }

    /// ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    pub async fn wait_for_completion(&self) -> Result<()> {
        let timeout = Duration::from_secs(30);
        let start_time = Instant::now();

        loop {
            let stats = self.get_stats();
            if stats.total_tasks > 0
                && stats.completed_tasks + stats.failed_tasks >= stats.total_tasks
            {
                break;
            }

            if start_time.elapsed() > timeout {
                return Err(anyhow::anyhow!("ç­‰å¾…ä»»åŠ¡å®Œæˆè¶…æ—¶"));
            }

            tokio::time::sleep(Duration::from_millis(100)).await;
        }
        Ok(())
    }

    /// ä¼˜é›…å…³é—­æ‰§è¡Œå™¨
    pub async fn shutdown(mut self) -> Result<()> {
        info!("ğŸ›‘ å¼€å§‹å…³é—­é«˜æ€§èƒ½æ‰§è¡Œå™¨ (MPMC)");

        if let Some(shutdown_tx) = self.shutdown_tx.take() {
            let _ = shutdown_tx.send(());
        }

        // ç­‰å¾…æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆ
        self.wait_for_completion().await?;

        // ç­‰å¾…å·¥ä½œçº¿ç¨‹é€€å‡º
        let handles = {
            let mut handles = self.worker_handles.write().unwrap();
            std::mem::take(&mut *handles)
        };

        for handle in handles {
            if let Err(e) = handle.await {
                error!("å·¥ä½œçº¿ç¨‹é€€å‡ºé”™è¯¯: {}", e);
            }
        }

        info!("ğŸ›‘ é«˜æ€§èƒ½æ‰§è¡Œå™¨ (MPMC) å·²å…³é—­");
        Ok(())
    }
}
