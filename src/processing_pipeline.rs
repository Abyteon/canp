//! # æ•°æ®å¤„ç†ç®¡é“ (Processing Pipeline)
//! 
//! æ•´åˆæ‰€æœ‰ç»„ä»¶çš„å®Œæ•´æ•°æ®å¤„ç†ç®¡é“
//! 
//! ## å¤„ç†æµç¨‹
//! 1. æ–‡ä»¶æ‰¹é‡æ˜ å°„å’ŒåŠ è½½
//! 2. 4å±‚æ•°æ®ç»“æ„è§£æ
//! 3. DBCä¿¡å·è§£æ
//! 4. åˆ—å¼å­˜å‚¨è¾“å‡º
//! 5. ç»Ÿè®¡å’Œç›‘æ§
//! 
//! ## æ ¸å¿ƒç‰¹æ€§
//! - é›¶æ‹·è´æ€§èƒ½ä¼˜åŒ–
//! - é«˜å¹¶å‘å¤„ç†
//! - æ™ºèƒ½æ‰¹å¤„ç†
//! - é”™è¯¯æ¢å¤
//! - å®Œæ•´ç›‘æ§

use anyhow::{Result, Context};
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::Semaphore;
use tracing::{info, warn, error, debug};
use futures::stream::{self, StreamExt};

use crate::zero_copy_memory_pool::{ZeroCopyMemoryPool, MemoryPoolConfig};
use crate::high_performance_executor::{HighPerformanceExecutor, ExecutorConfig};
use crate::data_layer_parser::{DataLayerParser, ParsingStats as DataParsingStats};
use crate::dbc_parser::{DbcManager, DbcManagerConfig, DbcParsingStats};
use crate::columnar_storage::{ColumnarStorageWriter, ColumnarStorageConfig, StorageStats};

/// å¤„ç†ç®¡é“é…ç½®
#[derive(Debug, Clone)]
pub struct PipelineConfig {
    /// å†…å­˜æ± é…ç½®
    pub memory_pool_config: MemoryPoolConfig,
    /// æ‰§è¡Œå™¨é…ç½®
    pub executor_config: ExecutorConfig,
    /// DBCç®¡ç†å™¨é…ç½®
    pub dbc_manager_config: DbcManagerConfig,
    /// åˆ—å¼å­˜å‚¨é…ç½®
    pub storage_config: ColumnarStorageConfig,
    /// æ‰¹å¤„ç†å¤§å°
    pub batch_size: usize,
    /// æœ€å¤§å¹¶å‘æ–‡ä»¶æ•°
    pub max_concurrent_files: usize,
    /// æ˜¯å¦å¯ç”¨é”™è¯¯æ¢å¤
    pub enable_error_recovery: bool,
    /// é”™è¯¯é‡è¯•æ¬¡æ•°
    pub max_retries: usize,
    /// å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pub processing_timeout_seconds: u64,
    /// å†…å­˜å‹åŠ›é˜ˆå€¼
    pub memory_pressure_threshold: f64,
    /// æ˜¯å¦å¯ç”¨è¿›åº¦æŠ¥å‘Š
    pub enable_progress_reporting: bool,
    /// è¿›åº¦æŠ¥å‘Šé—´éš”ï¼ˆç§’ï¼‰
    pub progress_report_interval: u64,
}

impl Default for PipelineConfig {
    fn default() -> Self {
        Self {
            memory_pool_config: MemoryPoolConfig::default(),
            executor_config: ExecutorConfig::default(),
            dbc_manager_config: DbcManagerConfig::default(),
            storage_config: ColumnarStorageConfig::default(),
            batch_size: 50,  // 50ä¸ªæ–‡ä»¶ä¸€æ‰¹
            max_concurrent_files: num_cpus::get() * 2,
            enable_error_recovery: true,
            max_retries: 3,
            processing_timeout_seconds: 300, // 5åˆ†é’Ÿ
            memory_pressure_threshold: 0.8, // 80%
            enable_progress_reporting: true,
            progress_report_interval: 30, // 30ç§’
        }
    }
}

/// æ–‡ä»¶å¤„ç†ç»“æœ
#[derive(Debug)]
pub struct FileProcessingResult {
    /// æ–‡ä»¶è·¯å¾„
    pub file_path: PathBuf,
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// é”™è¯¯ä¿¡æ¯
    pub error: Option<String>,
    /// å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub processing_time_ms: u64,
    /// è§£æçš„æ¶ˆæ¯æ•°
    pub parsed_messages: usize,
    /// è§£æçš„ä¿¡å·æ•°  
    pub parsed_signals: usize,
    /// æ–‡ä»¶å¤§å°
    pub file_size: u64,
    /// å¤„ç†ååé‡ï¼ˆMB/sï¼‰
    pub throughput_mb_s: f64,
}

/// æ‰¹å¤„ç†ç»“æœ
#[derive(Debug)]
pub struct BatchProcessingResult {
    /// æ‰¹æ¬¡ID
    pub batch_id: usize,
    /// æ–‡ä»¶ç»“æœåˆ—è¡¨
    pub file_results: Vec<FileProcessingResult>,
    /// æ‰¹å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub batch_time_ms: u64,
    /// æˆåŠŸæ–‡ä»¶æ•°
    pub successful_files: usize,
    /// å¤±è´¥æ–‡ä»¶æ•°
    pub failed_files: usize,
    /// æ‰¹æ¬¡ååé‡ï¼ˆMB/sï¼‰
    pub batch_throughput_mb_s: f64,
}

/// ç®¡é“å¤„ç†ç»Ÿè®¡
#[derive(Debug, Default, Clone)]
pub struct PipelineStats {
    /// æ€»æ–‡ä»¶æ•°
    pub total_files: usize,
    /// æˆåŠŸå¤„ç†æ–‡ä»¶æ•°
    pub successful_files: usize,
    /// å¤±è´¥æ–‡ä»¶æ•°
    pub failed_files: usize,
    /// è·³è¿‡æ–‡ä»¶æ•°
    pub skipped_files: usize,
    /// é‡è¯•æ–‡ä»¶æ•°
    pub retried_files: usize,
    /// æ€»å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub total_processing_time_ms: u64,
    /// å¹³å‡æ–‡ä»¶å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub avg_file_processing_time_ms: f64,
    /// æ€»ååé‡ï¼ˆMB/sï¼‰
    pub total_throughput_mb_s: f64,
    /// å³°å€¼å†…å­˜ä½¿ç”¨ï¼ˆå­—èŠ‚ï¼‰
    pub peak_memory_usage: usize,
    /// æ•°æ®è§£æç»Ÿè®¡
    pub data_parsing_stats: DataParsingStats,
    /// DBCè§£æç»Ÿè®¡
    pub dbc_parsing_stats: DbcParsingStats,
    /// å­˜å‚¨ç»Ÿè®¡
    pub storage_stats: StorageStats,
}

impl PipelineStats {
    /// æ‰“å°è¯¦ç»†ç»Ÿè®¡
    pub fn print_detailed_summary(&self) {
        info!("ğŸ¯ æ•°æ®å¤„ç†ç®¡é“å®Œæ•´ç»Ÿè®¡:");
        info!("{}", "=".repeat(60));
        
        // åŸºæœ¬ç»Ÿè®¡
        info!("ğŸ“Š æ–‡ä»¶å¤„ç†ç»Ÿè®¡:");
        info!("  ğŸ“ æ€»æ–‡ä»¶æ•°: {}", self.total_files);
        info!("  âœ… æˆåŠŸå¤„ç†: {} ({:.1}%)", 
            self.successful_files, 
            if self.total_files > 0 { 
                self.successful_files as f64 / self.total_files as f64 * 100.0 
            } else { 0.0 }
        );
        info!("  âŒ å¤„ç†å¤±è´¥: {} ({:.1}%)", 
            self.failed_files,
            if self.total_files > 0 { 
                self.failed_files as f64 / self.total_files as f64 * 100.0 
            } else { 0.0 }
        );
        info!("  â­ï¸ è·³è¿‡æ–‡ä»¶: {}", self.skipped_files);
        info!("  ğŸ”„ é‡è¯•æ–‡ä»¶: {}", self.retried_files);
        
        // æ€§èƒ½ç»Ÿè®¡
        info!("ğŸš€ æ€§èƒ½ç»Ÿè®¡:");
        info!("  â±ï¸ æ€»å¤„ç†æ—¶é—´: {:.2} åˆ†é’Ÿ", self.total_processing_time_ms as f64 / 60000.0);
        info!("  ğŸ“ˆ å¹³å‡æ–‡ä»¶å¤„ç†æ—¶é—´: {:.2} ç§’", self.avg_file_processing_time_ms / 1000.0);
        info!("  ğŸŒŠ æ€»ååé‡: {:.2} MB/s", self.total_throughput_mb_s);
        info!("  ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {:.2} GB", self.peak_memory_usage as f64 / 1024.0 / 1024.0 / 1024.0);
        
        info!("{}", "=".repeat(60));
        
        // è¯¦ç»†å­æ¨¡å—ç»Ÿè®¡
        info!("ğŸ“‹ æ•°æ®è§£æç»Ÿè®¡:");
        self.data_parsing_stats.print_summary();
        
        info!("ğŸ“¡ DBCè§£æç»Ÿè®¡:");
        self.dbc_parsing_stats.print_summary();
        
        info!("ğŸ’½ å­˜å‚¨ç»Ÿè®¡:");
        self.storage_stats.print_summary();
    }
}

/// æ•°æ®å¤„ç†ç®¡é“
pub struct DataProcessingPipeline {
    /// é…ç½®
    config: PipelineConfig,
    /// å†…å­˜æ± 
    memory_pool: Arc<ZeroCopyMemoryPool>,
    /// é«˜æ€§èƒ½æ‰§è¡Œå™¨
    executor: Arc<HighPerformanceExecutor>,
    /// DBCç®¡ç†å™¨
    dbc_manager: Arc<DbcManager>,
    /// æ•°æ®è§£æå™¨
    data_parser: Arc<tokio::sync::Mutex<DataLayerParser>>,
    /// åˆ—å¼å­˜å‚¨å†™å…¥å™¨
    storage_writer: Arc<tokio::sync::Mutex<ColumnarStorageWriter>>,
    /// å¹¶å‘æ§åˆ¶ä¿¡å·é‡
    semaphore: Arc<Semaphore>,
    /// ç®¡é“ç»Ÿè®¡
    stats: Arc<tokio::sync::RwLock<PipelineStats>>,
}

impl DataProcessingPipeline {
    /// åˆ›å»ºæ–°çš„æ•°æ®å¤„ç†ç®¡é“
    pub async fn new(config: PipelineConfig) -> Result<Self> {
        info!("ğŸš€ åˆå§‹åŒ–æ•°æ®å¤„ç†ç®¡é“...");
        
        // åˆ›å»ºå†…å­˜æ± 
        let memory_pool = Arc::new(ZeroCopyMemoryPool::new(config.memory_pool_config.clone()));
        info!("âœ… å†…å­˜æ± åˆå§‹åŒ–å®Œæˆ");
        
        // åˆ›å»ºé«˜æ€§èƒ½æ‰§è¡Œå™¨
        let executor = Arc::new(HighPerformanceExecutor::new(config.executor_config.clone()));
        info!("âœ… é«˜æ€§èƒ½æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ");
        
        // åˆ›å»ºDBCç®¡ç†å™¨
        let dbc_manager = Arc::new(DbcManager::new(config.dbc_manager_config.clone()));
        info!("âœ… DBCç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ");
        
        // åˆ›å»ºæ•°æ®è§£æå™¨
        let data_parser = Arc::new(tokio::sync::Mutex::new(
            DataLayerParser::new((*memory_pool).clone())
        ));
        info!("âœ… æ•°æ®è§£æå™¨åˆå§‹åŒ–å®Œæˆ");
        
        // åˆ›å»ºåˆ—å¼å­˜å‚¨å†™å…¥å™¨
        let storage_writer = Arc::new(tokio::sync::Mutex::new(
            ColumnarStorageWriter::new(config.storage_config.clone())?
        ));
        info!("âœ… åˆ—å¼å­˜å‚¨å†™å…¥å™¨åˆå§‹åŒ–å®Œæˆ");
        
        // åˆ›å»ºå¹¶å‘æ§åˆ¶
        let semaphore = Arc::new(Semaphore::new(config.max_concurrent_files));
        
        // åˆå§‹åŒ–ç»Ÿè®¡
        let stats = Arc::new(tokio::sync::RwLock::new(PipelineStats::default()));
        
        info!("ğŸ‰ æ•°æ®å¤„ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ");
        
        Ok(Self {
            config,
            memory_pool,
            executor,
            dbc_manager,
            data_parser,
            storage_writer,
            semaphore,
            stats,
        })
    }
    
    /// åŠ è½½DBCæ–‡ä»¶
    pub async fn load_dbc_files<P: AsRef<Path>>(&self, dbc_paths: Vec<P>) -> Result<()> {
        info!("ğŸ“„ å¼€å§‹åŠ è½½DBCæ–‡ä»¶ï¼Œæ•°é‡: {}", dbc_paths.len());
        
        for (index, path) in dbc_paths.iter().enumerate() {
            let priority = -(index as i32); // æŒ‰é¡ºåºè®¾ç½®ä¼˜å…ˆçº§
            self.dbc_manager.load_dbc_file(path, Some(priority)).await
                .with_context(|| format!("åŠ è½½DBCæ–‡ä»¶å¤±è´¥: {:?}", path.as_ref()))?;
        }
        
        info!("âœ… æ‰€æœ‰DBCæ–‡ä»¶åŠ è½½å®Œæˆ");
        Ok(())
    }
    
    /// åŠ è½½DBCç›®å½•
    pub async fn load_dbc_directory<P: AsRef<Path>>(&self, dbc_dir: P) -> Result<usize> {
        info!("ğŸ“ ä»ç›®å½•åŠ è½½DBCæ–‡ä»¶: {:?}", dbc_dir.as_ref());
        
        let loaded_count = self.dbc_manager.load_dbc_directory(dbc_dir).await?;
        
        info!("âœ… ä»ç›®å½•åŠ è½½å®Œæˆï¼ŒæˆåŠŸåŠ è½½ {} ä¸ªDBCæ–‡ä»¶", loaded_count);
        Ok(loaded_count)
    }
    
    /// å¤„ç†æ–‡ä»¶åˆ—è¡¨
    pub async fn process_files<P: AsRef<Path>>(&self, file_paths: Vec<P>) -> Result<Vec<BatchProcessingResult>> {
        let total_files = file_paths.len();
        info!("ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶ï¼Œæ€»æ•°: {}", total_files);
        
        // æ›´æ–°ç»Ÿè®¡ - åŸºäºtokioå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
        // ä¼˜åŒ–ï¼šå‡å°‘é”æ“ä½œï¼Œæé«˜æ€§èƒ½
        {
            let mut stats = self.stats.write().await;
            stats.total_files = total_files;
            stats.successful_files = 0;
            stats.failed_files = 0;
            stats.skipped_files = 0;
            stats.retried_files = 0;
        }
        
        // å¯åŠ¨è¿›åº¦æŠ¥å‘Šä»»åŠ¡
        let progress_handle = if self.config.enable_progress_reporting {
            Some(self.start_progress_reporting().await)
        } else {
            None
        };
        
        let start_time = Instant::now();
        
        // åˆ†æ‰¹å¤„ç†æ–‡ä»¶
        let batches: Vec<Vec<PathBuf>> = file_paths
            .into_iter()
            .map(|p| p.as_ref().to_path_buf())
            .collect::<Vec<_>>()
            .chunks(self.config.batch_size)
            .map(|chunk| chunk.to_vec())
            .collect();
        
        info!("ğŸ“¦ æ–‡ä»¶åˆ†æ‰¹å®Œæˆï¼Œæ‰¹æ¬¡æ•°: {}", batches.len());
        
        let mut batch_results = Vec::new();
        
        // å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        for (batch_id, batch_files) in batches.into_iter().enumerate() {
            info!("ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {}, æ–‡ä»¶æ•°: {}", batch_id + 1, batch_files.len());
            
            let batch_files_len = batch_files.len();
            let batch_result = self.process_file_batch(batch_id, batch_files).await
                .with_context(|| format!("æ‰¹æ¬¡ {} å¤„ç†å¤±è´¥ï¼Œæ–‡ä»¶æ•°: {}", batch_id + 1, batch_files_len))?;
            
            batch_results.push(batch_result);
            
            // æ£€æŸ¥å†…å­˜å‹åŠ›
            if self.check_memory_pressure().await {
                warn!("å†…å­˜å‹åŠ›è¿‡é«˜ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶");
                // å†…å­˜å‹åŠ›ç¼“è§£ï¼ˆç®€åŒ–å®ç°ï¼‰
                tokio::task::yield_now().await;
            }
        }
        
        // å®Œæˆå­˜å‚¨å†™å…¥
        {
            let mut writer = self.storage_writer.lock().await;
            writer.finish().await?;
        }
        
        // åœæ­¢è¿›åº¦æŠ¥å‘Š
        if let Some(handle) = progress_handle {
            handle.abort();
        }
        
        // æ›´æ–°æ€»ä½“ç»Ÿè®¡ - åŸºäºtokioå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
        // ä¼˜åŒ–ï¼šå‡å°‘é”æ“ä½œï¼Œæé«˜æ€§èƒ½
        let total_time = start_time.elapsed();
        {
            let mut stats = self.stats.write().await;
            stats.total_processing_time_ms = total_time.as_millis() as u64;
            
            // ä¼˜åŒ–å¹³å‡æ—¶é—´è®¡ç®—ï¼Œä½¿ç”¨æ•´æ•°è¿ç®—é¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜
            if stats.successful_files > 0 {
                let total_time_ms = stats.total_processing_time_ms as u128;
                let successful_files = stats.successful_files as u128;
                stats.avg_file_processing_time_ms = (total_time_ms / successful_files) as f64;
            }
            
            // ä»å­æ¨¡å—æ›´æ–°ç»Ÿè®¡ - ä¼˜åŒ–é”çš„ä½¿ç”¨
            {
                let data_parser = self.data_parser.lock().await;
                stats.data_parsing_stats = data_parser.get_stats().clone();
            }
            
            stats.dbc_parsing_stats = self.dbc_manager.get_stats();
            
            {
                let writer = self.storage_writer.lock().await;
                stats.storage_stats = writer.get_stats().clone();
            }
        }
        
        // æ‰“å°æœ€ç»ˆç»Ÿè®¡
        let final_stats = self.stats.read().await;
        final_stats.print_detailed_summary();
        
        info!("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼");
        Ok(batch_results)
    }
    
    /// å¤„ç†å•ä¸ªæ–‡ä»¶æ‰¹æ¬¡
    async fn process_file_batch(&self, batch_id: usize, file_paths: Vec<PathBuf>) -> Result<BatchProcessingResult> {
        let batch_start = Instant::now();
        
        // åˆ›å»ºä»»åŠ¡æµ
        let results = stream::iter(file_paths.into_iter().enumerate())
            .map(|(file_index, path)| {
                let pipeline = self;
                async move {
                    let _permit = pipeline.semaphore.acquire().await.unwrap();
                    pipeline.process_single_file(path, file_index).await
                }
            })
            .buffer_unordered(self.config.max_concurrent_files)
            .collect::<Vec<_>>()
            .await;
        
        let batch_time = batch_start.elapsed();
        
        // æ±‡æ€»æ‰¹æ¬¡ç»“æœ
        let mut successful_files = 0;
        let mut failed_files = 0;
        let mut total_size = 0u64;
        let mut file_results = Vec::new();
        
        for result in results {
            match result {
                Ok(file_result) => {
                    if file_result.success {
                        successful_files += 1;
                    } else {
                        failed_files += 1;
                    }
                    total_size += file_result.file_size;
                    file_results.push(file_result);
                }
                Err(e) => {
                    error!("æ–‡ä»¶å¤„ç†ä»»åŠ¡å¤±è´¥: {}", e);
                    failed_files += 1;
                }
            }
        }
        
        // è®¡ç®—æ‰¹æ¬¡ååé‡ - åŸºäºtokioå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
        // ä¼˜åŒ–ï¼šä½¿ç”¨æ•´æ•°è¿ç®—é¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜
        let batch_throughput_mb_s = if batch_time.as_secs() > 0 {
            let total_size_mb = total_size / (1024 * 1024);
            total_size_mb as f64 / batch_time.as_secs() as f64
        } else {
            0.0
        };
        
        // æ›´æ–°ç»Ÿè®¡
        {
            let mut stats = self.stats.write().await;
            stats.successful_files += successful_files;
            stats.failed_files += failed_files;
        }
        
        info!("âœ… æ‰¹æ¬¡ {} å¤„ç†å®Œæˆ: æˆåŠŸ {}, å¤±è´¥ {}, ååé‡ {:.2} MB/s", 
            batch_id + 1, successful_files, failed_files, batch_throughput_mb_s);
        
        Ok(BatchProcessingResult {
            batch_id,
            file_results,
            batch_time_ms: batch_time.as_millis() as u64,
            successful_files,
            failed_files,
            batch_throughput_mb_s,
        })
    }
    
    /// å¤„ç†å•ä¸ªæ–‡ä»¶
    async fn process_single_file(&self, file_path: PathBuf, file_index: usize) -> Result<FileProcessingResult> {
        let start_time = Instant::now();
        let mut retry_count = 0;
        
        loop {
            match self.process_single_file_impl(&file_path, file_index).await {
                Ok(result) => return Ok(result),
                Err(e) => {
                    if retry_count < self.config.max_retries && self.config.enable_error_recovery {
                        retry_count += 1;
                        warn!("æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œé‡è¯• {}/{}: {:?} - {}", 
                            retry_count, self.config.max_retries, file_path, e);
                        
                        // æ›´æ–°é‡è¯•ç»Ÿè®¡
                        {
                            let mut stats = self.stats.write().await;
                            stats.retried_files += 1;
                        }
                        
                        // ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯• - åŸºäºtokioå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
                        // ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œé¿å…é¢‘ç¹é‡è¯•
                        let backoff_duration = std::cmp::min(
                            1000 * (1 << retry_count) as u64, // æŒ‡æ•°é€€é¿
                            10000 // æœ€å¤§ç­‰å¾…10ç§’
                        );
                        tokio::time::sleep(tokio::time::Duration::from_millis(backoff_duration)).await;
                        continue;
                    } else {
                        error!("æ–‡ä»¶å¤„ç†æœ€ç»ˆå¤±è´¥: {:?} - {}", file_path, e);
                        return Ok(FileProcessingResult {
                            file_path,
                            success: false,
                            error: Some(e.to_string()),
                            processing_time_ms: start_time.elapsed().as_millis() as u64,
                            parsed_messages: 0,
                            parsed_signals: 0,
                            file_size: 0,
                            throughput_mb_s: 0.0,
                        });
                    }
                }
            }
        }
    }
    
    /// å®é™…å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®ç°
    async fn process_single_file_impl(&self, file_path: &PathBuf, _file_index: usize) -> Result<FileProcessingResult> {
        let start_time = Instant::now();
        
        debug!("ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶: {:?}", file_path);
        
        // 1. åˆ›å»ºæ–‡ä»¶æ˜ å°„
        let file_mapping = self.memory_pool.create_file_mapping(file_path)
            .context("åˆ›å»ºæ–‡ä»¶æ˜ å°„å¤±è´¥")?;
        
        let file_size = file_mapping.len() as u64;
        
        // 2. è§£æ4å±‚æ•°æ®ç»“æ„
        let parsed_data = {
            let mut parser = self.data_parser.lock().await;
            parser.parse_file(file_mapping.as_slice()).await
                .context("æ•°æ®ç»“æ„è§£æå¤±è´¥")?
        };
        
        // 3. DBCä¿¡å·è§£æ
        let mut parsed_messages = Vec::new();
        let mut total_signals = 0;
        
        for sequence in &parsed_data.frame_sequences {
            for frame in &sequence.frames {
                if let Some(parsed_message) = self.dbc_manager.parse_can_frame(frame).await? {
                    total_signals += parsed_message.signals.len();
                    parsed_messages.push(parsed_message);
                }
            }
        }
        
        // 4. åˆ—å¼å­˜å‚¨å†™å…¥
        {
            let mut writer = self.storage_writer.lock().await;
            writer.write_parsed_data(&parsed_data, &parsed_messages, file_path).await
                .context("åˆ—å¼å­˜å‚¨å†™å…¥å¤±è´¥")?;
        }
        
        let processing_time = start_time.elapsed();
        let throughput_mb_s = if processing_time.as_secs_f64() > 0.0 {
            (file_size as f64 / 1024.0 / 1024.0) / processing_time.as_secs_f64()
        } else {
            0.0
        };
        
        debug!("âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {:?}, æ¶ˆæ¯æ•°: {}, ä¿¡å·æ•°: {}, ååé‡: {:.2} MB/s", 
            file_path, parsed_messages.len(), total_signals, throughput_mb_s);
        
        Ok(FileProcessingResult {
            file_path: file_path.clone(),
            success: true,
            error: None,
            processing_time_ms: processing_time.as_millis() as u64,
            parsed_messages: parsed_messages.len(),
            parsed_signals: total_signals,
            file_size,
            throughput_mb_s,
        })
    }
    
    /// æ£€æŸ¥å†…å­˜å‹åŠ› - åŸºäºtokioå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
    async fn check_memory_pressure(&self) -> bool {
        // åŸºäºç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µæ£€æŸ¥å†…å­˜å‹åŠ›
        // è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„å®ç°ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç›‘æ§
        let mut system = sysinfo::System::new_all();
        system.refresh_memory();
        let memory_usage = system.used_memory() as f64 / system.total_memory() as f64;
        memory_usage > self.config.memory_pressure_threshold
    }
    
    /// å¯åŠ¨è¿›åº¦æŠ¥å‘Šä»»åŠ¡
    async fn start_progress_reporting(&self) -> tokio::task::JoinHandle<()> {
        let stats = Arc::clone(&self.stats);
        let interval = self.config.progress_report_interval;
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(interval));
            
            loop {
                interval.tick().await;
                
                let stats = stats.read().await;
                if stats.total_files > 0 {
                    let progress = (stats.successful_files + stats.failed_files) as f64 / stats.total_files as f64 * 100.0;
                    info!("ğŸ“Š å¤„ç†è¿›åº¦: {:.1}% ({}/{}) - æˆåŠŸ: {}, å¤±è´¥: {}", 
                        progress,
                        stats.successful_files + stats.failed_files,
                        stats.total_files,
                        stats.successful_files,
                        stats.failed_files
                    );
                }
            }
        })
    }
    
    /// è·å–ç®¡é“ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_stats(&self) -> PipelineStats {
        self.stats.read().await.clone()
    }
    
    /// é‡ç½®ç»Ÿè®¡ä¿¡æ¯
    pub async fn reset_stats(&self) {
        let mut stats = self.stats.write().await;
        *stats = PipelineStats::default();
        
        // é‡ç½®å­æ¨¡å—ç»Ÿè®¡
        {
            let mut parser = self.data_parser.lock().await;
            parser.reset_stats();
        }
        
        self.dbc_manager.reset_stats();
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use crate::test_data_generator::{TestDataGenerator, TestDataConfig};
    
    #[tokio::test]
    async fn test_pipeline_creation() {
        let temp_dir = TempDir::new().unwrap();
        
        let config = PipelineConfig {
            storage_config: ColumnarStorageConfig {
                output_dir: temp_dir.path().to_path_buf(),
                ..ColumnarStorageConfig::default()
            },
            ..PipelineConfig::default()
        };
        
        let pipeline = DataProcessingPipeline::new(config).await.unwrap();
        let stats = pipeline.get_stats().await;
        
        assert_eq!(stats.total_files, 0);
        assert_eq!(stats.successful_files, 0);
    }
    
    #[tokio::test]
    async fn test_pipeline_processing() {
        let temp_dir = TempDir::new().unwrap();
        
        // ç”Ÿæˆæµ‹è¯•æ•°æ®
        let data_config = TestDataConfig {
            file_count: 2,
            target_file_size: 1024 * 1024, // 1MB
            frames_per_file: 100,
            output_dir: temp_dir.path().join("test_data"),
        };
        
        let generator = TestDataGenerator::new(data_config);
        let file_paths = generator.generate_all().await.unwrap();
        
        // é…ç½®ç®¡é“
        let pipeline_config = PipelineConfig {
            storage_config: ColumnarStorageConfig {
                output_dir: temp_dir.path().join("output"),
                ..ColumnarStorageConfig::default()
            },
            batch_size: 2,
            max_concurrent_files: 2,
            ..PipelineConfig::default()
        };
        
        let pipeline = DataProcessingPipeline::new(pipeline_config).await.unwrap();
        
        // å¤„ç†æ–‡ä»¶
        let results = pipeline.process_files(file_paths).await.unwrap();
        
        assert!(!results.is_empty());
        
        let stats = pipeline.get_stats().await;
        assert_eq!(stats.total_files, 2);
        
        stats.print_detailed_summary();
    }
}