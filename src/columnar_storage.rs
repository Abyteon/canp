//! # åˆ—å¼å­˜å‚¨æ¨¡å— (Columnar Storage)
//! 
//! é«˜æ€§èƒ½åˆ—å¼å­˜å‚¨è¾“å‡ºï¼Œæ”¯æŒParquetæ ¼å¼
//! 
//! ## æ ¸å¿ƒåŠŸèƒ½
//! - CANæ•°æ®åˆ—å¼å­˜å‚¨
//! - å¤´éƒ¨ä¿¡æ¯ä¿ç•™
//! - é«˜å‹ç¼©ç‡è¾“å‡º
//! - åˆ†åŒºç­–ç•¥æ”¯æŒ
//! - ç´¢å¼•å’Œå…ƒæ•°æ®ç®¡ç†
//! 
//! ## å­˜å‚¨æ ¼å¼
//! - ä½¿ç”¨Apache Parquetæ ¼å¼
//! - æ”¯æŒå¤šç§å‹ç¼©ç®—æ³•
//! - è‡ªåŠ¨åˆ†åŒºç®¡ç†
//! - ä¸°å¯Œçš„å…ƒæ•°æ®ä¿¡æ¯

use anyhow::{Result, Context};
use arrow::array::*;
use arrow::datatypes::*;
use arrow::record_batch::RecordBatch;
use chrono::{DateTime, Utc};
use parquet::arrow::ArrowWriter;
use parquet::basic::Compression;
use parquet::file::properties::WriterProperties;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs::File;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tracing::{info, debug};

use crate::data_layer_parser::ParsedFileData;
use crate::dbc_parser::{ParsedMessage, ParsedSignal};

/// åˆ—å¼å­˜å‚¨é…ç½®
#[derive(Debug, Clone)]
pub struct ColumnarStorageConfig {
    /// è¾“å‡ºç›®å½•
    pub output_dir: PathBuf,
    /// å‹ç¼©ç®—æ³•
    pub compression: CompressionType,
    /// è¡Œç»„å¤§å°
    pub row_group_size: usize,
    /// é¡µé¢å¤§å°
    pub page_size: usize,
    /// æ˜¯å¦å¯ç”¨å­—å…¸ç¼–ç 
    pub enable_dictionary: bool,
    /// æ˜¯å¦å¯ç”¨ç»Ÿè®¡ä¿¡æ¯
    pub enable_statistics: bool,
    /// åˆ†åŒºç­–ç•¥
    pub partition_strategy: PartitionStrategy,
    /// æ‰¹é‡å†™å…¥å¤§å°
    pub batch_size: usize,
    /// æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆå­—èŠ‚ï¼‰
    pub max_file_size: usize,
    /// æ˜¯å¦ä¿ç•™åŸå§‹æ•°æ®
    pub keep_raw_data: bool,
}

/// å‹ç¼©ç±»å‹
#[derive(Debug, Clone, Copy)]
pub enum CompressionType {
    None,
    Snappy,
    Gzip,
    Lzo,
    Brotli,
    Lz4,
    Zstd,
}

impl From<CompressionType> for Compression {
    fn from(compression: CompressionType) -> Self {
        match compression {
            CompressionType::None => Compression::UNCOMPRESSED,
            CompressionType::Snappy => Compression::SNAPPY,
            CompressionType::Gzip => Compression::GZIP(Default::default()),
            CompressionType::Lzo => Compression::LZO,
            CompressionType::Brotli => Compression::BROTLI(Default::default()),
            CompressionType::Lz4 => Compression::LZ4,
            CompressionType::Zstd => Compression::ZSTD(Default::default()),
        }
    }
}

/// åˆ†åŒºç­–ç•¥
#[derive(Debug, Clone)]
pub enum PartitionStrategy {
    /// ä¸åˆ†åŒº
    None,
    /// æŒ‰æ—¶é—´åˆ†åŒºï¼ˆå°æ—¶ï¼‰
    Hourly,
    /// æŒ‰æ—¶é—´åˆ†åŒºï¼ˆå¤©ï¼‰
    Daily,
    /// æŒ‰æ–‡ä»¶åˆ†åŒº
    ByFile,
    /// æŒ‰CAN IDåˆ†åŒº
    ByCanId,
    /// è‡ªå®šä¹‰åˆ†åŒºå‡½æ•°
    Custom(fn(&ParsedFileData) -> String),
}

impl Default for ColumnarStorageConfig {
    fn default() -> Self {
        Self {
            output_dir: PathBuf::from("output"),
            compression: CompressionType::Zstd,
            row_group_size: 10000,
            page_size: 1024 * 1024, // 1MB
            enable_dictionary: true,
            enable_statistics: true,
            partition_strategy: PartitionStrategy::Daily,
            batch_size: 1000,
            max_file_size: 100 * 1024 * 1024, // 100MB
            keep_raw_data: false,
        }
    }
}

/// æ–‡ä»¶å…ƒæ•°æ®ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileMetadata {
    /// åŸå§‹æ–‡ä»¶è·¯å¾„
    pub source_file: String,
    /// æ–‡ä»¶ç´¢å¼•
    pub file_index: u32,
    /// æ–‡ä»¶ç‰ˆæœ¬
    pub file_version: u32,
    /// å¤„ç†æ—¶é—´æˆ³
    pub processed_timestamp: u64,
    /// æ–‡ä»¶å¤§å°
    pub file_size: u64,
    /// å‹ç¼©æ•°æ®é•¿åº¦
    pub compressed_length: u32,
    /// æ€»å¸§æ•°
    pub total_frames: usize,
    /// æ€»æ¶ˆæ¯æ•°
    pub total_messages: usize,
    /// å”¯ä¸€CAN IDåˆ—è¡¨
    pub unique_can_ids: Vec<u32>,
    /// DBCæ–‡ä»¶åˆ—è¡¨
    pub dbc_files: Vec<String>,
    /// å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    pub processing_stats: ProcessingStats,
}

/// å¤„ç†ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingStats {
    /// æˆåŠŸè§£æçš„æ¶ˆæ¯æ•°
    pub successful_messages: usize,
    /// æœªçŸ¥æ¶ˆæ¯æ•°
    pub unknown_messages: usize,
    /// è§£æé”™è¯¯æ•°
    pub parse_errors: usize,
    /// å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub processing_time_ms: u64,
    /// å‹ç¼©æ¯”
    pub compression_ratio: f64,
}

/// å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Default, Clone)]
pub struct StorageStats {
    /// å¤„ç†çš„æ–‡ä»¶æ•°
    pub files_processed: usize,
    /// å†™å…¥çš„è¡Œæ•°
    pub rows_written: usize,
    /// å†™å…¥çš„å­—èŠ‚æ•°
    pub bytes_written: usize,
    /// è¾“å‡ºæ–‡ä»¶æ•°
    pub output_files: usize,
    /// å‹ç¼©åå¤§å°
    pub compressed_size: usize,
    /// åŸå§‹æ•°æ®å¤§å°
    pub raw_size: usize,
    /// å¹³å‡å‹ç¼©æ¯”
    pub avg_compression_ratio: f64,
    /// å†™å…¥æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub write_time_ms: u64,
}

impl StorageStats {
    /// æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    pub fn print_summary(&self) {
        info!("ğŸ“Š åˆ—å¼å­˜å‚¨ç»Ÿè®¡:");
        info!("  ğŸ“ å¤„ç†æ–‡ä»¶æ•°: {}", self.files_processed);
        info!("  ğŸ“‹ å†™å…¥è¡Œæ•°: {}", self.rows_written);
        info!("  ğŸ’¾ å†™å…¥å­—èŠ‚æ•°: {:.2} MB", self.bytes_written as f64 / 1024.0 / 1024.0);
        info!("  ğŸ“„ è¾“å‡ºæ–‡ä»¶æ•°: {}", self.output_files);
        info!("  ğŸ—œï¸ å‹ç¼©åå¤§å°: {:.2} MB", self.compressed_size as f64 / 1024.0 / 1024.0);
        info!("  ğŸ“¦ åŸå§‹æ•°æ®å¤§å°: {:.2} MB", self.raw_size as f64 / 1024.0 / 1024.0);
        info!("  ğŸ“ˆ å¹³å‡å‹ç¼©æ¯”: {:.2}%", self.avg_compression_ratio * 100.0);
        info!("  â±ï¸ å†™å…¥æ—¶é—´: {:.2} ç§’", self.write_time_ms as f64 / 1000.0);
    }
}

/// åˆ—å¼å­˜å‚¨å†™å…¥å™¨
pub struct ColumnarStorageWriter {
    /// é…ç½®
    config: ColumnarStorageConfig,
    /// å½“å‰åˆ†åŒºå†™å…¥å™¨
    current_writers: HashMap<String, PartitionWriter>,
    /// å­˜å‚¨ç»Ÿè®¡
    stats: StorageStats,
    /// è¾“å‡ºSchema
    schema: Arc<Schema>,
}

/// åˆ†åŒºå†™å…¥å™¨
struct PartitionWriter {
    /// æ–‡ä»¶è·¯å¾„
    file_path: PathBuf,
    /// Arrowå†™å…¥å™¨
    writer: ArrowWriter<File>,
    /// å½“å‰æ‰¹æ¬¡æ•°æ®
    batch_data: BatchData,
    /// å†™å…¥è¡Œæ•°
    rows_written: usize,
    /// æ–‡ä»¶å¤§å°
    file_size: usize,
}

/// æ‰¹æ¬¡æ•°æ®ç´¯ç§¯å™¨
#[derive(Debug)]
struct BatchData {
    // æ–‡ä»¶çº§åˆ«ä¿¡æ¯
    source_files: Vec<String>,
    file_indices: Vec<u32>,
    file_versions: Vec<u32>,
    file_timestamps: Vec<u64>,
    
    // æ¶ˆæ¯çº§åˆ«ä¿¡æ¯
    message_timestamps: Vec<u64>,
    can_ids: Vec<u32>,
    message_names: Vec<String>,
    dlcs: Vec<u8>,
    senders: Vec<Option<String>>,
    
    // åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
    raw_data: Vec<Vec<u8>>,
    
    // ä¿¡å·æ•°æ®ï¼ˆåŠ¨æ€ç»“æ„ï¼‰
    signal_data: HashMap<String, SignalColumn>,
    
    // DBCä¿¡æ¯
    dbc_sources: Vec<String>,
    
    // æ‰¹æ¬¡å¤§å°
    batch_size: usize,
}

/// ä¿¡å·åˆ—æ•°æ®
#[derive(Debug)]
struct SignalColumn {
    /// ä¿¡å·åç§°
    name: String,
    /// åŸå§‹å€¼
    raw_values: Vec<Option<u64>>,
    /// ç‰©ç†å€¼
    physical_values: Vec<Option<f64>>,
    /// å•ä½
    units: Vec<Option<String>>,
    /// å€¼è¡¨æè¿°
    value_descriptions: Vec<Option<String>>,
}

impl ColumnarStorageWriter {
    /// åˆ›å»ºæ–°çš„åˆ—å¼å­˜å‚¨å†™å…¥å™¨
    pub fn new(config: ColumnarStorageConfig) -> Result<Self> {
        // åˆ›å»ºè¾“å‡ºç›®å½•
        std::fs::create_dir_all(&config.output_dir)
            .context("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥")?;
        
        // å®šä¹‰è¾“å‡ºSchema
        let schema = Self::create_schema(config.keep_raw_data);
        
        Ok(Self {
            config,
            current_writers: HashMap::new(),
            stats: StorageStats::default(),
            schema,
        })
    }
    
    /// åˆ›å»ºParquet Schema
    fn create_schema(keep_raw_data: bool) -> Arc<Schema> {
        let mut fields = vec![
            // æ–‡ä»¶çº§åˆ«å­—æ®µ
            Field::new("source_file", DataType::Utf8, false),
            Field::new("file_index", DataType::UInt32, false),
            Field::new("file_version", DataType::UInt32, false),
            Field::new("file_timestamp", DataType::UInt64, false),
            
            // æ¶ˆæ¯çº§åˆ«å­—æ®µ
            Field::new("message_timestamp", DataType::UInt64, false),
            Field::new("can_id", DataType::UInt32, false),
            Field::new("message_name", DataType::Utf8, true),
            Field::new("dlc", DataType::UInt8, false),
            Field::new("sender", DataType::Utf8, true),
            
            // DBCä¿¡æ¯
            Field::new("dbc_source", DataType::Utf8, true),
            
            // ä¿¡å·æ•°æ®å°†åŠ¨æ€æ·»åŠ 
            Field::new("signal_name", DataType::Utf8, true),
            Field::new("signal_raw_value", DataType::UInt64, true),
            Field::new("signal_physical_value", DataType::Float64, true),
            Field::new("signal_unit", DataType::Utf8, true),
            Field::new("signal_description", DataType::Utf8, true),
        ];
        
        // å¯é€‰çš„åŸå§‹æ•°æ®å­—æ®µ
        if keep_raw_data {
            fields.push(Field::new("raw_data", DataType::Binary, true));
        }
        
        Arc::new(Schema::new(fields))
    }
    
    /// å†™å…¥è§£æå®Œæˆçš„æ–‡ä»¶æ•°æ®
    pub async fn write_parsed_data(
        &mut self,
        parsed_data: &ParsedFileData,
        parsed_messages: &[ParsedMessage],
        source_path: &Path
    ) -> Result<()> {
        
        let start_time = std::time::Instant::now();
        
        // ç¡®å®šåˆ†åŒº
        let partition_key = self.get_partition_key(parsed_data);
        
        // è·å–æˆ–åˆ›å»ºåˆ†åŒºå†™å…¥å™¨
        if !self.current_writers.contains_key(&partition_key) {
            self.create_partition_writer(&partition_key).await?;
        }
        
        // å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        let batch_data = self.prepare_batch_data(
            parsed_data,
            parsed_messages,
            source_path
        )?;
        
        // æ·»åŠ åˆ°å¯¹åº”åˆ†åŒºçš„å†™å…¥å™¨
        let writer = self.current_writers.get_mut(&partition_key).unwrap();
        writer.add_batch_data(batch_data)?;
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°æ‰¹æ¬¡
        if writer.should_flush(&self.config) {
            self.flush_partition_writer(&partition_key).await?;
        }
        
        // æ›´æ–°ç»Ÿè®¡
        self.stats.files_processed += 1;
        self.stats.rows_written += parsed_messages.len();
        self.stats.write_time_ms += start_time.elapsed().as_millis() as u64;
        
        debug!("å†™å…¥æ–‡ä»¶æ•°æ®å®Œæˆ: {:?}, æ¶ˆæ¯æ•°: {}", source_path, parsed_messages.len());
        
        Ok(())
    }
    
    /// ç¡®å®šåˆ†åŒºé”®
    fn get_partition_key(&self, parsed_data: &ParsedFileData) -> String {
        match &self.config.partition_strategy {
            PartitionStrategy::None => "default".to_string(),
            PartitionStrategy::Hourly => {
                let datetime = DateTime::from_timestamp(parsed_data.file_header.timestamp as i64, 0)
                    .unwrap_or_else(|| DateTime::<Utc>::from_timestamp(0, 0).unwrap());
                format!("hour={}", datetime.format("%Y%m%d%H"))
            }
            PartitionStrategy::Daily => {
                let datetime = DateTime::from_timestamp(parsed_data.file_header.timestamp as i64, 0)
                    .unwrap_or_else(|| DateTime::<Utc>::from_timestamp(0, 0).unwrap());
                format!("day={}", datetime.format("%Y%m%d"))
            }
            PartitionStrategy::ByFile => {
                format!("file={}", parsed_data.file_header.file_index)
            }
            PartitionStrategy::ByCanId => {
                let can_ids = parsed_data.unique_can_ids();
                if can_ids.len() == 1 {
                    format!("can_id={:08X}", can_ids[0])
                } else {
                    "mixed_can_ids".to_string()
                }
            }
            PartitionStrategy::Custom(func) => {
                func(parsed_data)
            }
        }
    }
    
    /// åˆ›å»ºåˆ†åŒºå†™å…¥å™¨
    async fn create_partition_writer(&mut self, partition_key: &str) -> Result<()> {
        // æ„é€ æ–‡ä»¶è·¯å¾„
        let file_name = format!("data_{}.parquet", 
            chrono::Utc::now().format("%Y%m%d_%H%M%S_%f"));
        
        let partition_dir = self.config.output_dir.join(partition_key);
        std::fs::create_dir_all(&partition_dir)
            .context("åˆ›å»ºåˆ†åŒºç›®å½•å¤±è´¥")?;
        
        let file_path = partition_dir.join(file_name);
        
        // åˆ›å»ºå†™å…¥å™¨å±æ€§
        let props = WriterProperties::builder()
            .set_compression(self.config.compression.into())
            .set_dictionary_enabled(self.config.enable_dictionary)
            .set_statistics_enabled(if self.config.enable_statistics { 
                parquet::file::properties::EnabledStatistics::Chunk 
            } else { 
                parquet::file::properties::EnabledStatistics::None 
            })
            .set_max_row_group_size(self.config.row_group_size)
            .set_data_page_size_limit(self.config.page_size)
            .build();
        
        // åˆ›å»ºæ–‡ä»¶å’Œå†™å…¥å™¨
        let file = File::create(&file_path)
            .context("åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤±è´¥")?;
        
        let writer = ArrowWriter::try_new(file, self.schema.clone(), Some(props))
            .context("åˆ›å»ºArrowå†™å…¥å™¨å¤±è´¥")?;
        
        // åˆ›å»ºåˆ†åŒºå†™å…¥å™¨
        let partition_writer = PartitionWriter {
            file_path: file_path.clone(),
            writer,
            batch_data: BatchData::new(self.config.batch_size),
            rows_written: 0,
            file_size: 0,
        };
        
        self.current_writers.insert(partition_key.to_string(), partition_writer);
        self.stats.output_files += 1;
        
        info!("åˆ›å»ºåˆ†åŒºå†™å…¥å™¨: {} -> {:?}", partition_key, file_path);
        
        Ok(())
    }
    
    /// å‡†å¤‡æ‰¹æ¬¡æ•°æ®
    fn prepare_batch_data(
        &self,
        parsed_data: &ParsedFileData,
        parsed_messages: &[ParsedMessage],
        source_path: &Path
    ) -> Result<Vec<RecordBatch>> {
        
        let mut batches = Vec::new();
        let mut current_batch = BatchData::new(self.config.batch_size);
        
        // éå†æ‰€æœ‰è§£æçš„æ¶ˆæ¯
        for message in parsed_messages {
            // éå†æ¶ˆæ¯ä¸­çš„æ‰€æœ‰ä¿¡å·
            for signal in &message.signals {
                // æ·»åŠ åŸºç¡€æ•°æ®
                current_batch.add_signal_data(
                    parsed_data,
                    message,
                    signal,
                    source_path
                )?;
                
                // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹æ¬¡å¤§å°
                if current_batch.is_full() {
                    batches.push(current_batch.to_record_batch(&self.schema)?);
                    current_batch = BatchData::new(self.config.batch_size);
                }
            }
        }
        
        // å¤„ç†å‰©ä½™æ•°æ®
        if !current_batch.is_empty() {
            batches.push(current_batch.to_record_batch(&self.schema)?);
        }
        
        Ok(batches)
    }
    
    /// åˆ·æ–°åˆ†åŒºå†™å…¥å™¨
    async fn flush_partition_writer(&mut self, partition_key: &str) -> Result<()> {
        if let Some(mut writer) = self.current_writers.remove(partition_key) {
            writer.flush().await?;
            
            // æ›´æ–°ç»Ÿè®¡
            self.stats.bytes_written += writer.file_size;
            
            info!("åˆ†åŒºå†™å…¥å™¨åˆ·æ–°å®Œæˆ: {}, å†™å…¥è¡Œæ•°: {}", 
                partition_key, writer.rows_written);
        }
        
        Ok(())
    }
    
    /// å®Œæˆæ‰€æœ‰å†™å…¥
    pub async fn finish(&mut self) -> Result<()> {
        info!("å¼€å§‹å®Œæˆæ‰€æœ‰å†™å…¥å™¨...");
        
        let partition_keys: Vec<String> = self.current_writers.keys().cloned().collect();
        
        for partition_key in partition_keys {
            self.flush_partition_writer(&partition_key).await?;
        }
        
        // å†™å…¥å…ƒæ•°æ®æ–‡ä»¶
        self.write_metadata().await?;
        
        info!("æ‰€æœ‰å†™å…¥å™¨å®Œæˆ");
        self.stats.print_summary();
        
        Ok(())
    }
    
    /// å†™å…¥å…ƒæ•°æ®æ–‡ä»¶
    async fn write_metadata(&self) -> Result<()> {
        let metadata_path = self.config.output_dir.join("_metadata.json");
        
        let metadata = serde_json::json!({
            "created_at": chrono::Utc::now().to_rfc3339(),
            "schema": format!("{:?}", self.schema), // ç®€åŒ–å¤„ç†
            "config": {
                "compression": format!("{:?}", self.config.compression),
                "partition_strategy": format!("{:?}", self.config.partition_strategy),
                "row_group_size": self.config.row_group_size,
                "batch_size": self.config.batch_size,
            },
            "stats": {
                "files_processed": self.stats.files_processed,
                "rows_written": self.stats.rows_written,
                "bytes_written": self.stats.bytes_written,
                "output_files": self.stats.output_files,
                "write_time_ms": self.stats.write_time_ms,
            }
        });
        
        tokio::fs::write(&metadata_path, serde_json::to_string_pretty(&metadata)?)
            .await
            .context("å†™å…¥å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥")?;
        
        debug!("å…ƒæ•°æ®æ–‡ä»¶å†™å…¥å®Œæˆ: {:?}", metadata_path);
        
        Ok(())
    }
    
    /// è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> &StorageStats {
        &self.stats
    }
}

impl BatchData {
    /// åˆ›å»ºæ–°çš„æ‰¹æ¬¡æ•°æ®
    fn new(batch_size: usize) -> Self {
        Self {
            source_files: Vec::with_capacity(batch_size),
            file_indices: Vec::with_capacity(batch_size),
            file_versions: Vec::with_capacity(batch_size),
            file_timestamps: Vec::with_capacity(batch_size),
            message_timestamps: Vec::with_capacity(batch_size),
            can_ids: Vec::with_capacity(batch_size),
            message_names: Vec::with_capacity(batch_size),
            dlcs: Vec::with_capacity(batch_size),
            senders: Vec::with_capacity(batch_size),
            raw_data: Vec::with_capacity(batch_size),
            signal_data: HashMap::new(),
            dbc_sources: Vec::with_capacity(batch_size),
            batch_size,
        }
    }
    
    /// æ·»åŠ ä¿¡å·æ•°æ®
    fn add_signal_data(
        &mut self,
        parsed_data: &ParsedFileData,
        message: &ParsedMessage,
        signal: &ParsedSignal,
        source_path: &Path
    ) -> Result<()> {
        
        // æ·»åŠ åŸºç¡€æ•°æ®
        self.source_files.push(source_path.to_string_lossy().to_string());
        self.file_indices.push(parsed_data.file_header.file_index);
        self.file_versions.push(parsed_data.file_header.version);
        self.file_timestamps.push(parsed_data.file_header.timestamp);
        
        self.message_timestamps.push(message.parsed_timestamp);
        self.can_ids.push(message.message_id);
        self.message_names.push(message.name.clone());
        self.dlcs.push(message.dlc);
        self.senders.push(message.sender.clone());
        self.dbc_sources.push(signal.source_dbc.to_string_lossy().to_string());
        
        // æ·»åŠ åŸå§‹æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.raw_data.push(Vec::new()); // è¿™é‡Œéœ€è¦ä»messageä¸­è·å–åŸå§‹æ•°æ®
        
        Ok(())
    }
    
    /// è½¬æ¢ä¸ºRecordBatch
    fn to_record_batch(&self, schema: &Arc<Schema>) -> Result<RecordBatch> {
        let mut arrays: Vec<ArrayRef> = Vec::new();
        
        // æ„å»ºåŸºç¡€å­—æ®µæ•°ç»„
        arrays.push(Arc::new(StringArray::from(self.source_files.clone())));
        arrays.push(Arc::new(UInt32Array::from(self.file_indices.clone())));
        arrays.push(Arc::new(UInt32Array::from(self.file_versions.clone())));
        arrays.push(Arc::new(UInt64Array::from(self.file_timestamps.clone())));
        arrays.push(Arc::new(UInt64Array::from(self.message_timestamps.clone())));
        arrays.push(Arc::new(UInt32Array::from(self.can_ids.clone())));
        arrays.push(Arc::new(StringArray::from(self.message_names.clone())));
        arrays.push(Arc::new(UInt8Array::from(self.dlcs.clone())));
        
        // å¤„ç†å¯é€‰å­—æ®µ
        let sender_array: ArrayRef = Arc::new(
            self.senders.iter()
                .map(|s| s.as_deref())
                .collect::<StringArray>()
        );
        arrays.push(sender_array);
        
        arrays.push(Arc::new(StringArray::from(self.dbc_sources.clone())));
        
        // æ·»åŠ ä¿¡å·ç›¸å…³çš„å ä½ç¬¦å­—æ®µ
        arrays.push(Arc::new(StringArray::from(vec![""; self.source_files.len()])));  // signal_name
        arrays.push(Arc::new(UInt64Array::from(vec![0u64; self.source_files.len()]))); // signal_raw_value
        arrays.push(Arc::new(Float64Array::from(vec![0.0f64; self.source_files.len()]))); // signal_physical_value
        arrays.push(Arc::new(StringArray::from(vec![""; self.source_files.len()])));  // signal_unit
        arrays.push(Arc::new(StringArray::from(vec![""; self.source_files.len()])));  // signal_description
        
        RecordBatch::try_new(schema.clone(), arrays)
            .context("åˆ›å»ºRecordBatchå¤±è´¥")
    }
    
    /// æ£€æŸ¥æ˜¯å¦å·²æ»¡
    fn is_full(&self) -> bool {
        self.source_files.len() >= self.batch_size
    }
    
    /// æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    fn is_empty(&self) -> bool {
        self.source_files.is_empty()
    }
}

impl PartitionWriter {
    /// æ·»åŠ æ‰¹æ¬¡æ•°æ®
    fn add_batch_data(&mut self, batches: Vec<RecordBatch>) -> Result<()> {
        for batch in batches {
            self.rows_written += batch.num_rows();
            // å®é™…çš„æ‰¹æ¬¡æ•°æ®ä¼šåœ¨è¿™é‡Œæ·»åŠ åˆ°self.batch_dataä¸­
            // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥å†™å…¥
        }
        Ok(())
    }
    
    /// æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
    fn should_flush(&self, config: &ColumnarStorageConfig) -> bool {
        self.rows_written >= config.batch_size || 
        self.file_size >= config.max_file_size
    }
    
    /// åˆ·æ–°æ•°æ®
    async fn flush(&mut self) -> Result<()> {
        // åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„writeræ¥æ›¿æ¢ï¼Œä»¥ä¾¿å¯ä»¥è°ƒç”¨close
        let temp_file = tempfile::NamedTempFile::new()?;
        let temp_writer = ArrowWriter::try_new(
            temp_file.into_file(), 
            arrow::datatypes::SchemaRef::new(arrow::datatypes::Schema::empty()), 
            None
        )?;
        
        let old_writer = std::mem::replace(&mut self.writer, temp_writer);
        old_writer.close().context("å…³é—­Arrowå†™å…¥å™¨å¤±è´¥")?;
        
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use crate::test_data_generator::{TestDataGenerator, TestDataConfig};
    use crate::data_layer_parser::DataLayerParser;
    use crate::zero_copy_memory_pool::ZeroCopyMemoryPool;
    
    #[tokio::test]
    async fn test_columnar_storage_writer() {
        let temp_dir = TempDir::new().unwrap();
        
        // é…ç½®åˆ—å¼å­˜å‚¨
        let config = ColumnarStorageConfig {
            output_dir: temp_dir.path().to_path_buf(),
            compression: CompressionType::Snappy,
            batch_size: 100,
            ..ColumnarStorageConfig::default()
        };
        
        let mut writer = ColumnarStorageWriter::new(config).unwrap();
        
        // éªŒè¯åŸºæœ¬åŠŸèƒ½
        assert_eq!(writer.stats.files_processed, 0);
        assert_eq!(writer.stats.rows_written, 0);
        
        // å®Œæˆå†™å…¥
        writer.finish().await.unwrap();
        
        // æ£€æŸ¥è¾“å‡ºç›®å½•
        assert!(temp_dir.path().join("_metadata.json").exists());
    }
    
    #[test]
    fn test_partition_strategy() {
        let writer = ColumnarStorageWriter::new(ColumnarStorageConfig::default()).unwrap();
        
        // åˆ›å»ºæµ‹è¯•æ•°æ®
        use crate::data_layer_parser::{FileHeader, DecompressedHeader};
        
        let file_header = FileHeader {
            magic: *b"CANDATA\0",
            version: 1,
            file_index: 123,
            timestamp: 1640995200, // 2022-01-01 00:00:00 UTC
            crc32: 0,
            compressed_length: 1000,
            reserved: [0; 3],
        };
        
        let parsed_data = ParsedFileData {
            file_header,
            decompressed_header: DecompressedHeader {
                data_type: *b"FRAM",
                version: 2,
                total_frames: 100,
                file_index: 123,
                data_length: 2400,
            },
            frame_sequences: Vec::new(),
        };
        
        let partition_key = writer.get_partition_key(&parsed_data);
        assert!(partition_key.contains("day=20220101"));
    }
}